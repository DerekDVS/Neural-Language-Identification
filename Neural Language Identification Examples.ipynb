{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18af9d80",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c302eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2561591200.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2561591200.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2561591200.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2561591200.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2561591200.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2561591200.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(lang_trim)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in full dataset\n",
    "data = pd.read_csv('data/sentences.csv',\n",
    "                            sep='\\t', \n",
    "                            encoding='utf8', \n",
    "                            index_col=0,\n",
    "                            names=['lang','text'])\n",
    "\n",
    "#Filter by text length\n",
    "len_cond = [True if 20<=len(s)<=200 else False for s in data['text']]\n",
    "data = data[len_cond]\n",
    "\n",
    "#Filter by text language\n",
    "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "data = data[data['lang'].isin(lang)]\n",
    "\n",
    "#Select 50000 rows for each language\n",
    "data_trim = pd.DataFrame(columns=['lang','text'])\n",
    "\n",
    "for l in lang:\n",
    "    lang_trim = data[data['lang'] ==l].sample(50000,random_state = 100)\n",
    "    data_trim = data_trim.append(lang_trim)\n",
    "\n",
    "#Create a random train, valid, test split\n",
    "data_shuffle = data_trim.sample(frac=1)\n",
    "\n",
    "train = data_shuffle[0:210000]\n",
    "valid = data_shuffle[210000:270000]\n",
    "test = data_shuffle[270000:300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4f0be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_trigrams(corpus,n_feat=200):\n",
    "    \"\"\"\n",
    "    Returns a list of the N most common character trigrams from a list of sentences\n",
    "    params\n",
    "    ------------\n",
    "        corpus: list of strings\n",
    "        n_feat: integer\n",
    "    \"\"\"\n",
    "    \n",
    "    #fit the n-gram model\n",
    "    vectorizer = CountVectorizer(analyzer='char',\n",
    "                            ngram_range=(3, 3)\n",
    "                            ,max_features=n_feat)\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    #Get model feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1782dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain trigrams from each language\n",
    "features = {}\n",
    "features_set = set()\n",
    "\n",
    "for l in lang:\n",
    "    \n",
    "    #get corpus filtered by language\n",
    "    corpus = train[train.lang==l]['text']\n",
    "    \n",
    "    #get 200 most frequent trigrams\n",
    "    trigrams = get_trigrams(corpus)\n",
    "    \n",
    "    #add to dict and set\n",
    "    features[l] = trigrams \n",
    "    features_set.update(trigrams)\n",
    "\n",
    "    \n",
    "#create vocabulary list using feature set\n",
    "vocab = dict()\n",
    "for i,f in enumerate(features_set):\n",
    "    vocab[f]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7258af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train count vectoriser using vocabulary\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "                             ngram_range=(3, 3),\n",
    "                            vocabulary=vocab)\n",
    "\n",
    "#create feature matrix for training set\n",
    "corpus = train['text']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fbbda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale feature matrix \n",
    "train_min = train_feat.min()\n",
    "train_max = train_feat.max()\n",
    "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
    "\n",
    "#Add target variable \n",
    "train_feat['lang'] = list(train['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46f82ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature matrix for validation set\n",
    "corpus = valid['text']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "valid_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
    "valid_feat = (valid_feat - train_min)/(train_max-train_min)\n",
    "valid_feat['lang'] = list(valid['lang'])\n",
    "\n",
    "#create feature matrix for test set\n",
    "corpus = test['text']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "test_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
    "test_feat = (test_feat - train_min)/(train_max-train_min)\n",
    "test_feat['lang'] = list(test['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b57f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Fit encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['deu', 'eng', 'fra', 'ita', 'por', 'spa'])\n",
    "\n",
    "def encode(y):\n",
    "    \"\"\"\n",
    "    Returns a list of one hot encodings\n",
    "    Params\n",
    "    ---------\n",
    "        y: list of language labels\n",
    "    \"\"\"\n",
    "    \n",
    "    y_encoded = encoder.transform(y)\n",
    "    y_dummy = np_utils.to_categorical(y_encoded)\n",
    "    \n",
    "    return y_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b614be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4075959, 2)\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 663), found shape=(100, 667)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Train model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecqyk8e9i.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Derek\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 663), found shape=(100, 667)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "#Get training data\n",
    "x = train_feat.drop('lang',axis=1)\n",
    "y = encode(train_feat['lang'])\n",
    "\n",
    "#Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=663, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Train model\n",
    "model.fit(x, y, epochs=4, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d99b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_feat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Get predictions on test set\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(x_test)\n\u001b[0;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minverse_transform(labels)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Accuracy on test set\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "x_test = test_feat.drop('lang',axis=1)\n",
    "y_test = test_feat['lang']\n",
    "\n",
    "#Get predictions on test set\n",
    "labels = model.predict_classes(x_test)\n",
    "predictions = encoder.inverse_transform(labels)\n",
    "\n",
    "#Accuracy on test set\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "print(accuracy)\n",
    "\n",
    "#Create confusion matrix\n",
    "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "conf_matrix = confusion_matrix(y_test,predictions)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,columns=lang,index=lang)\n",
    "\n",
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8b7e5",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fff0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9381771\n",
      "401\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmn</td>\n",
       "      <td>今天是６月１８号，也是Muiriel的生日！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cmn</td>\n",
       "      <td>选择什么是“对”或“错”是一项艰难的任务，我们却必须要完成它。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>cmn</td>\n",
       "      <td>我们看东西不是看其实质，而是以我们的主观意识看它们的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>cmn</td>\n",
       "      <td>生活就是當你忙著進行你的計劃時總有其他的事情發生。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>deu</td>\n",
       "      <td>Lass uns etwas versuchen!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang                             sent\n",
       "4   cmn           今天是６月１８号，也是Muiriel的生日！\n",
       "20  cmn  选择什么是“对”或“错”是一项艰难的任务，我们却必须要完成它。\n",
       "66  cmn      我们看东西不是看其实质，而是以我们的主观意识看它们的。\n",
       "70  cmn        生活就是當你忙著進行你的計劃時總有其他的事情發生。\n",
       "75  deu        Lass uns etwas versuchen!"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_original = pd.read_csv('data/sentences.csv',sep='\\t', encoding='utf8', header = None)\n",
    "data_original.drop([0], axis=1,inplace=True)\n",
    "data_original.columns = ['lang','sent']\n",
    "data_original = data_original[data_original.sent.str.len()<=200]#Only select sentences less that 200 characters\n",
    "data_original = data_original[data_original.sent.str.len()>=20]#Only select sentences greater that 20 characters\n",
    "print(len(data_original)) #5788767 rows\n",
    "print(data_original['lang'].nunique()) #314 unique languages\n",
    "data_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba2f62fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4075959\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>deu</td>\n",
       "      <td>Lass uns etwas versuchen!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>deu</td>\n",
       "      <td>Ich muss schlafen gehen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>deu</td>\n",
       "      <td>Heute ist der 18. Juni und das ist der Geburts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>deu</td>\n",
       "      <td>Herzlichen Glückwunsch zum Geburtstag, Muiriel!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>deu</td>\n",
       "      <td>Muiriel ist jetzt 20.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang                                               sent\n",
       "75  deu                          Lass uns etwas versuchen!\n",
       "76  deu                           Ich muss schlafen gehen.\n",
       "78  deu  Heute ist der 18. Juni und das ist der Geburts...\n",
       "79  deu    Herzlichen Glückwunsch zum Geburtstag, Muiriel!\n",
       "80  deu                              Muiriel ist jetzt 20."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want English, German, Russian, Spanish, French, Japanese, Portuguese, Italian\n",
    "lang = ['eng','deu','spa','fra','por','ita']\n",
    "data = data_original[data_original['lang'].isin(lang)]\n",
    "print(len(data)) #2759972 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c363235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng    1589318\n",
       "ita     723534\n",
       "deu     571659\n",
       "fra     487634\n",
       "por     366601\n",
       "spa     337213\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dfaaee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2639099085.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2639099085.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2639099085.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2639099085.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2639099085.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n",
      "C:\\Users\\Derek\\AppData\\Local\\Temp\\ipykernel_10636\\2639099085.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eng    50000\n",
       "deu    50000\n",
       "spa    50000\n",
       "fra    50000\n",
       "por    50000\n",
       "ita    50000\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select 50000 rows for each language\n",
    "data_trim = pd.DataFrame(columns=['lang','sent'])\n",
    "data_trim\n",
    "for l in lang:\n",
    "    data_trim = data_trim.append(data[data['lang'] ==l].sample(50000,random_state = 100))\n",
    "data_trim['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9bad795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into training, validation and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_trim['sent']\n",
    "y = data_trim['lang']\n",
    "X_train, X, y_train, y = train_test_split(X, y, test_size=0.30, random_state=101)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X, y, test_size=1/3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db7132fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210000 60000 30000\n"
     ]
    }
   ],
   "source": [
    "#save datsets\n",
    "train = pd.concat([pd.Series(y_train),pd.Series(X_train) ], axis=1)\n",
    "valid = pd.concat([pd.Series(y_valid),pd.Series(X_valid) ], axis=1)\n",
    "test = pd.concat([pd.Series(y_test),pd.Series(X_test) ], axis=1)\n",
    "print(len(train), len(valid), len(test))\n",
    "\n",
    "train.to_csv('data/train.csv')\n",
    "valid.to_csv('data/valid.csv')\n",
    "test.to_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f88c49",
   "metadata": {},
   "source": [
    "## Create Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "61b465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47dc01ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>I guess we have no choice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>« Où est Tom ? » – « Il est à la maison. »</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deu</td>\n",
       "      <td>Er spricht unsere Sprache nicht.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>por</td>\n",
       "      <td>Eu conheço Tom melhor do que você conhece.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Ein edler Mensch widmet sich dem Erreichen hoh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                               sent\n",
       "0  eng                         I guess we have no choice.\n",
       "1  fra         « Où est Tom ? » – « Il est à la maison. »\n",
       "2  deu                   Er spricht unsere Sprache nicht.\n",
       "3  por         Eu conheço Tom melhor do que você conhece.\n",
       "4  deu  Ein edler Mensch widmet sich dem Erreichen hoh..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0799083a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello This function Removes numbers punctuation and normalizes spaces'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove punctuation and numbers\n",
    "def clean_text(sent):\n",
    "    \"Takes in a string and returns it with no numbers or punctuation and normalized spaces\"\n",
    "    remove=string.punctuation + \"1234567890\" #Characters to be removed\n",
    "    table=str.maketrans(\"\",\"\",remove)    \n",
    "    sent = sent.translate(table)  \n",
    "    sent = \" \".join(sent.split()) #Normalize spaces\n",
    "    return sent\n",
    "\n",
    "sent = \"Hello. #This function908 Removes  numbers12,    punctuation... and     normalizes spaces\"\n",
    "clean_text(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f7dfcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thi',\n",
       " 'his',\n",
       " 'is ',\n",
       " 's i',\n",
       " ' is',\n",
       " 'is ',\n",
       " 's a',\n",
       " ' a ',\n",
       " 'a s',\n",
       " ' se',\n",
       " 'sen',\n",
       " 'ent',\n",
       " 'nte',\n",
       " 'ten',\n",
       " 'enc',\n",
       " 'nce',\n",
       " 'ce.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_trigram(sent):\n",
    "    \"Takes a string and returns a list of character n-grams\"\n",
    "    return [sent[i:i+3] for i in range(len(sent)-3+1)]\n",
    "\n",
    "sent = \"This is a sentence.\"\n",
    "char_trigram(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d4eb175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I g',\n",
       " ' gu',\n",
       " 'gue',\n",
       " 'ues',\n",
       " 'ess',\n",
       " 'ss ',\n",
       " 's w',\n",
       " ' we',\n",
       " 'we ',\n",
       " 'e h',\n",
       " ' ha',\n",
       " 'hav',\n",
       " 'ave',\n",
       " 've ',\n",
       " 'e n',\n",
       " ' no',\n",
       " 'no ',\n",
       " 'o c',\n",
       " ' ch',\n",
       " 'cho',\n",
       " 'hoi',\n",
       " 'oic',\n",
       " 'ice',\n",
       " '« O',\n",
       " ' Où',\n",
       " 'Où ',\n",
       " 'ù e',\n",
       " ' es',\n",
       " 'est',\n",
       " 'st ',\n",
       " 't T',\n",
       " ' To',\n",
       " 'Tom',\n",
       " 'om ',\n",
       " 'm »',\n",
       " ' » ',\n",
       " '» –',\n",
       " ' – ',\n",
       " '– «',\n",
       " ' « ',\n",
       " '« I',\n",
       " ' Il',\n",
       " 'Il ',\n",
       " 'l e',\n",
       " ' es',\n",
       " 'est',\n",
       " 'st ',\n",
       " 't à',\n",
       " ' à ',\n",
       " 'à l',\n",
       " ' la',\n",
       " 'la ',\n",
       " 'a m',\n",
       " ' ma',\n",
       " 'mai',\n",
       " 'ais',\n",
       " 'iso',\n",
       " 'son',\n",
       " 'on ',\n",
       " 'n »',\n",
       " 'Er ',\n",
       " 'r s',\n",
       " ' sp',\n",
       " 'spr',\n",
       " 'pri',\n",
       " 'ric',\n",
       " 'ich',\n",
       " 'cht',\n",
       " 'ht ',\n",
       " 't u',\n",
       " ' un',\n",
       " 'uns',\n",
       " 'nse',\n",
       " 'ser',\n",
       " 'ere',\n",
       " 're ',\n",
       " 'e S',\n",
       " ' Sp',\n",
       " 'Spr',\n",
       " 'pra',\n",
       " 'rac',\n",
       " 'ach',\n",
       " 'che',\n",
       " 'he ',\n",
       " 'e n',\n",
       " ' ni',\n",
       " 'nic',\n",
       " 'ich',\n",
       " 'cht',\n",
       " 'Eu ',\n",
       " 'u c',\n",
       " ' co',\n",
       " 'con',\n",
       " 'onh',\n",
       " 'nhe',\n",
       " 'heç',\n",
       " 'eço',\n",
       " 'ço ',\n",
       " 'o T',\n",
       " ' To',\n",
       " 'Tom',\n",
       " 'om ',\n",
       " 'm m',\n",
       " ' me',\n",
       " 'mel',\n",
       " 'elh',\n",
       " 'lho',\n",
       " 'hor',\n",
       " 'or ',\n",
       " 'r d',\n",
       " ' do',\n",
       " 'do ',\n",
       " 'o q',\n",
       " ' qu',\n",
       " 'que',\n",
       " 'ue ',\n",
       " 'e v',\n",
       " ' vo',\n",
       " 'voc',\n",
       " 'ocê',\n",
       " 'cê ',\n",
       " 'ê c',\n",
       " ' co',\n",
       " 'con',\n",
       " 'onh',\n",
       " 'nhe',\n",
       " 'hec',\n",
       " 'ece',\n",
       " 'Ein',\n",
       " 'in ',\n",
       " 'n e',\n",
       " ' ed',\n",
       " 'edl',\n",
       " 'dle',\n",
       " 'ler',\n",
       " 'er ',\n",
       " 'r M',\n",
       " ' Me',\n",
       " 'Men',\n",
       " 'ens',\n",
       " 'nsc',\n",
       " 'sch',\n",
       " 'ch ',\n",
       " 'h w',\n",
       " ' wi',\n",
       " 'wid',\n",
       " 'idm',\n",
       " 'dme',\n",
       " 'met',\n",
       " 'et ',\n",
       " 't s',\n",
       " ' si',\n",
       " 'sic',\n",
       " 'ich',\n",
       " 'ch ',\n",
       " 'h d',\n",
       " ' de',\n",
       " 'dem',\n",
       " 'em ',\n",
       " 'm E',\n",
       " ' Er',\n",
       " 'Err',\n",
       " 'rre',\n",
       " 'rei',\n",
       " 'eic',\n",
       " 'ich',\n",
       " 'che',\n",
       " 'hen',\n",
       " 'en ',\n",
       " 'n h',\n",
       " ' ho',\n",
       " 'hoh',\n",
       " 'ohe',\n",
       " 'her',\n",
       " 'er ',\n",
       " 'r Z',\n",
       " ' Zi',\n",
       " 'Zie',\n",
       " 'iel',\n",
       " 'ele',\n",
       " 'Ela',\n",
       " 'la ',\n",
       " 'a f',\n",
       " ' fa',\n",
       " 'faz',\n",
       " 'az ',\n",
       " 'z p',\n",
       " ' pa',\n",
       " 'par',\n",
       " 'art',\n",
       " 'rte',\n",
       " 'te ',\n",
       " 'e d',\n",
       " ' do',\n",
       " 'do ',\n",
       " 'o c',\n",
       " ' cl',\n",
       " 'clu',\n",
       " 'lub',\n",
       " 'ube',\n",
       " 'be ',\n",
       " 'e d',\n",
       " ' de',\n",
       " 'de ',\n",
       " 'e t',\n",
       " ' tê',\n",
       " 'tên',\n",
       " 'êni',\n",
       " 'nis',\n",
       " 'Mei',\n",
       " 'ein',\n",
       " 'in ',\n",
       " 'n A',\n",
       " ' Au',\n",
       " 'Aut',\n",
       " 'uto',\n",
       " 'to ',\n",
       " 'o i',\n",
       " ' is',\n",
       " 'ist',\n",
       " 'st ',\n",
       " 't v',\n",
       " ' ve',\n",
       " 'ver',\n",
       " 'ers',\n",
       " 'rsi',\n",
       " 'sic',\n",
       " 'ich',\n",
       " 'che',\n",
       " 'her',\n",
       " 'ert',\n",
       " 'Mar',\n",
       " 'ary',\n",
       " 'ry ',\n",
       " 'y t',\n",
       " ' te',\n",
       " 'tem',\n",
       " 'em ',\n",
       " 'm u',\n",
       " ' um',\n",
       " 'um ',\n",
       " 'm n',\n",
       " ' na',\n",
       " 'nam',\n",
       " 'amo',\n",
       " 'mor',\n",
       " 'ora',\n",
       " 'rad',\n",
       " 'ado',\n",
       " 'do ',\n",
       " 'o s',\n",
       " ' se',\n",
       " 'sec',\n",
       " 'ecr',\n",
       " 'cre',\n",
       " 'ret',\n",
       " 'eto',\n",
       " 'Emi',\n",
       " 'mil',\n",
       " 'ily',\n",
       " 'ly ',\n",
       " 'y v',\n",
       " ' vi',\n",
       " 'vis',\n",
       " 'isi',\n",
       " 'sit',\n",
       " 'ita',\n",
       " 'tar',\n",
       " 'ará',\n",
       " 'rá ',\n",
       " 'á o',\n",
       " ' os',\n",
       " 'os ',\n",
       " 's p',\n",
       " ' pa',\n",
       " 'pai',\n",
       " 'ais',\n",
       " 'is ',\n",
       " 's d',\n",
       " ' de',\n",
       " 'del',\n",
       " 'ela',\n",
       " 'Auc',\n",
       " 'uch',\n",
       " 'ch ',\n",
       " 'h e',\n",
       " ' ei',\n",
       " 'ein',\n",
       " 'ine',\n",
       " 'ne ',\n",
       " 'e P',\n",
       " ' Ps',\n",
       " 'Psy',\n",
       " 'syc',\n",
       " 'ych',\n",
       " 'cho',\n",
       " 'hol',\n",
       " 'olo',\n",
       " 'log',\n",
       " 'ogi',\n",
       " 'gin',\n",
       " 'in ',\n",
       " 'n g',\n",
       " ' ge',\n",
       " 'ger',\n",
       " 'erä',\n",
       " 'rät',\n",
       " 'ät ',\n",
       " 't a',\n",
       " ' an',\n",
       " 'an ',\n",
       " 'n K',\n",
       " ' Ko',\n",
       " 'Kol',\n",
       " 'oll',\n",
       " 'lle',\n",
       " 'leg',\n",
       " 'ege',\n",
       " 'gen',\n",
       " 'en ',\n",
       " 'n d',\n",
       " ' di',\n",
       " 'die',\n",
       " 'ie ',\n",
       " 'e i',\n",
       " ' ih',\n",
       " 'ihr',\n",
       " 'hr ',\n",
       " 'r m',\n",
       " ' me',\n",
       " 'meh',\n",
       " 'ehr',\n",
       " 'hr ',\n",
       " 'r s',\n",
       " ' sc',\n",
       " 'sch',\n",
       " 'cha',\n",
       " 'had',\n",
       " 'ade',\n",
       " 'den',\n",
       " 'en ',\n",
       " 'n a',\n",
       " ' al',\n",
       " 'als',\n",
       " 'ls ',\n",
       " 's h',\n",
       " ' he',\n",
       " 'hel',\n",
       " 'elf',\n",
       " 'lfe',\n",
       " 'fen']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trigram_list(sent_list):\n",
    "    \"Takes in a list of sentences and returns a list of trigrams \"\n",
    "    sent_clean = list(map(clean_text, sent_list))\n",
    "    sent_trigram = list(map(char_trigram,sent_clean))\n",
    "    list_trigram = [item for sublist in sent_trigram for item in sublist]\n",
    "    return list_trigram\n",
    "\n",
    "sent_list = train['sent'][0:10]\n",
    "trigram_list(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a89420b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' de',\n",
       " 'er ',\n",
       " 'as ',\n",
       " 'en ',\n",
       " 'es ',\n",
       " ' qu',\n",
       " 'de ',\n",
       " 'que',\n",
       " 'te ',\n",
       " ' co',\n",
       " 're ',\n",
       " ' a ',\n",
       " 'est',\n",
       " 'Tom',\n",
       " 'om ',\n",
       " 'ue ',\n",
       " 'ch ',\n",
       " 'ent',\n",
       " ' pa',\n",
       " 'to ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_frequent(n,trigrams):\n",
    "    \"Takes in a list of trigrams and returns the n most frequent trigrams\"\n",
    "    common = []\n",
    "    for e in Counter(trigrams).most_common(n):\n",
    "        common.append(e[0])\n",
    "    return common\n",
    "\n",
    "sent_list = train['sent'][0:1000]\n",
    "trigrams = trigram_list(sent_list)\n",
    "most_frequent(20, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12b3e328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20': [' th',\n",
       "  'he ',\n",
       "  ' to',\n",
       "  'the',\n",
       "  'to ',\n",
       "  'om ',\n",
       "  'Tom',\n",
       "  'hat',\n",
       "  'nt ',\n",
       "  'ing',\n",
       "  'ed ',\n",
       "  'at ',\n",
       "  'is ',\n",
       "  'ng ',\n",
       "  'tha',\n",
       "  ' do',\n",
       "  ' wa',\n",
       "  ' yo',\n",
       "  'you',\n",
       "  'e t'],\n",
       " '30': [' th',\n",
       "  'he ',\n",
       "  ' to',\n",
       "  'the',\n",
       "  'to ',\n",
       "  'om ',\n",
       "  'Tom',\n",
       "  'hat',\n",
       "  'nt ',\n",
       "  'ing',\n",
       "  'ed ',\n",
       "  'at ',\n",
       "  'is ',\n",
       "  'ng ',\n",
       "  'tha',\n",
       "  ' do',\n",
       "  ' wa',\n",
       "  ' yo',\n",
       "  'you',\n",
       "  'e t',\n",
       "  're ',\n",
       "  ' a ',\n",
       "  ' an',\n",
       "  ' ha',\n",
       "  ' he',\n",
       "  'as ',\n",
       "  't t',\n",
       "  'er ',\n",
       "  'd t',\n",
       "  'nd ']}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lang_features(n_list,lang):\n",
    "    \"\"\"Returns a dictionary of the most frequent trigrams for a given language. Each element is a list of the n most\n",
    "    frequent trigrams when n is a element of n_list\"\"\"\n",
    "    \n",
    "    train_lang = train[train['lang'] == lang]\n",
    "    sent_list = train_lang['sent']\n",
    "    trigrams = trigram_list(sent_list)\n",
    "    \n",
    "    freq = {}\n",
    "    for n in n_list:\n",
    "        freq[n] = most_frequent(int(n), trigrams)\n",
    "    return freq \n",
    "    \n",
    "lang_features(['20','30'],'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13f19249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "deu\n",
      "spa\n",
      "fra\n",
      "por\n",
      "ita\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eng': {'50': [' th',\n",
       "   'he ',\n",
       "   ' to',\n",
       "   'the',\n",
       "   'to ',\n",
       "   'om ',\n",
       "   'Tom',\n",
       "   'hat',\n",
       "   'nt ',\n",
       "   'ing',\n",
       "   'ed ',\n",
       "   'at ',\n",
       "   'is ',\n",
       "   'ng ',\n",
       "   'tha',\n",
       "   ' do',\n",
       "   ' wa',\n",
       "   ' yo',\n",
       "   'you',\n",
       "   'e t',\n",
       "   're ',\n",
       "   ' a ',\n",
       "   ' an',\n",
       "   ' ha',\n",
       "   ' he',\n",
       "   'as ',\n",
       "   't t',\n",
       "   'er ',\n",
       "   'd t',\n",
       "   'nd ',\n",
       "   ' is',\n",
       "   ' in',\n",
       "   'ry ',\n",
       "   'her',\n",
       "   'ou ',\n",
       "   ' be',\n",
       "   'and',\n",
       "   've ',\n",
       "   'in ',\n",
       "   'thi',\n",
       "   ' of',\n",
       "   'll ',\n",
       "   'ary',\n",
       "   'e a',\n",
       "   'ere',\n",
       "   'Mar',\n",
       "   's a',\n",
       "   'was',\n",
       "   'e w',\n",
       "   ' To'],\n",
       "  '100': [' th',\n",
       "   'he ',\n",
       "   ' to',\n",
       "   'the',\n",
       "   'to ',\n",
       "   'om ',\n",
       "   'Tom',\n",
       "   'hat',\n",
       "   'nt ',\n",
       "   'ing',\n",
       "   'ed ',\n",
       "   'at ',\n",
       "   'is ',\n",
       "   'ng ',\n",
       "   'tha',\n",
       "   ' do',\n",
       "   ' wa',\n",
       "   ' yo',\n",
       "   'you',\n",
       "   'e t',\n",
       "   're ',\n",
       "   ' a ',\n",
       "   ' an',\n",
       "   ' ha',\n",
       "   ' he',\n",
       "   'as ',\n",
       "   't t',\n",
       "   'er ',\n",
       "   'd t',\n",
       "   'nd ',\n",
       "   ' is',\n",
       "   ' in',\n",
       "   'ry ',\n",
       "   'her',\n",
       "   'ou ',\n",
       "   ' be',\n",
       "   'and',\n",
       "   've ',\n",
       "   'in ',\n",
       "   'thi',\n",
       "   ' of',\n",
       "   'll ',\n",
       "   'ary',\n",
       "   'e a',\n",
       "   'ere',\n",
       "   'Mar',\n",
       "   's a',\n",
       "   'was',\n",
       "   'e w',\n",
       "   ' To',\n",
       "   'o t',\n",
       "   'me ',\n",
       "   'his',\n",
       "   's t',\n",
       "   'es ',\n",
       "   'of ',\n",
       "   ' co',\n",
       "   ' wi',\n",
       "   'e s',\n",
       "   ' hi',\n",
       "   'n t',\n",
       "   'ver',\n",
       "   'hin',\n",
       "   'The',\n",
       "   ' di',\n",
       "   'ld ',\n",
       "   ' wh',\n",
       "   ' Ma',\n",
       "   ' ca',\n",
       "   'on ',\n",
       "   ' go',\n",
       "   'ow ',\n",
       "   ' sa',\n",
       "   ' fo',\n",
       "   'en ',\n",
       "   ' we',\n",
       "   'for',\n",
       "   ' on',\n",
       "   'an ',\n",
       "   'ly ',\n",
       "   ' me',\n",
       "   't h',\n",
       "   'do ',\n",
       "   'ave',\n",
       "   ' wo',\n",
       "   'ent',\n",
       "   'ant',\n",
       "   ' sh',\n",
       "   ' re',\n",
       "   'id ',\n",
       "   ' st',\n",
       "   'oul',\n",
       "   ' li',\n",
       "   'all',\n",
       "   'e h',\n",
       "   ' no',\n",
       "   'or ',\n",
       "   'st ',\n",
       "   'e i',\n",
       "   'uld'],\n",
       "  '200': [' th',\n",
       "   'he ',\n",
       "   ' to',\n",
       "   'the',\n",
       "   'to ',\n",
       "   'om ',\n",
       "   'Tom',\n",
       "   'hat',\n",
       "   'nt ',\n",
       "   'ing',\n",
       "   'ed ',\n",
       "   'at ',\n",
       "   'is ',\n",
       "   'ng ',\n",
       "   'tha',\n",
       "   ' do',\n",
       "   ' wa',\n",
       "   ' yo',\n",
       "   'you',\n",
       "   'e t',\n",
       "   're ',\n",
       "   ' a ',\n",
       "   ' an',\n",
       "   ' ha',\n",
       "   ' he',\n",
       "   'as ',\n",
       "   't t',\n",
       "   'er ',\n",
       "   'd t',\n",
       "   'nd ',\n",
       "   ' is',\n",
       "   ' in',\n",
       "   'ry ',\n",
       "   'her',\n",
       "   'ou ',\n",
       "   ' be',\n",
       "   'and',\n",
       "   've ',\n",
       "   'in ',\n",
       "   'thi',\n",
       "   ' of',\n",
       "   'll ',\n",
       "   'ary',\n",
       "   'e a',\n",
       "   'ere',\n",
       "   'Mar',\n",
       "   's a',\n",
       "   'was',\n",
       "   'e w',\n",
       "   ' To',\n",
       "   'o t',\n",
       "   'me ',\n",
       "   'his',\n",
       "   's t',\n",
       "   'es ',\n",
       "   'of ',\n",
       "   ' co',\n",
       "   ' wi',\n",
       "   'e s',\n",
       "   ' hi',\n",
       "   'n t',\n",
       "   'ver',\n",
       "   'hin',\n",
       "   'The',\n",
       "   ' di',\n",
       "   'ld ',\n",
       "   ' wh',\n",
       "   ' Ma',\n",
       "   ' ca',\n",
       "   'on ',\n",
       "   ' go',\n",
       "   'ow ',\n",
       "   ' sa',\n",
       "   ' fo',\n",
       "   'en ',\n",
       "   ' we',\n",
       "   'for',\n",
       "   ' on',\n",
       "   'an ',\n",
       "   'ly ',\n",
       "   ' me',\n",
       "   't h',\n",
       "   'do ',\n",
       "   'ave',\n",
       "   ' wo',\n",
       "   'ent',\n",
       "   'ant',\n",
       "   ' sh',\n",
       "   ' re',\n",
       "   'id ',\n",
       "   ' st',\n",
       "   'oul',\n",
       "   ' li',\n",
       "   'all',\n",
       "   'e h',\n",
       "   ' no',\n",
       "   'or ',\n",
       "   'st ',\n",
       "   'e i',\n",
       "   'uld',\n",
       "   'ut ',\n",
       "   'are',\n",
       "   'ts ',\n",
       "   'one',\n",
       "   'g t',\n",
       "   't w',\n",
       "   't a',\n",
       "   ' I ',\n",
       "   'ont',\n",
       "   'now',\n",
       "   'ot ',\n",
       "   'ter',\n",
       "   'our',\n",
       "   ' kn',\n",
       "   'ght',\n",
       "   ' ne',\n",
       "   'eve',\n",
       "   'e d',\n",
       "   'th ',\n",
       "   ' so',\n",
       "   ' ar',\n",
       "   'e c',\n",
       "   'ome',\n",
       "   'hav',\n",
       "   'did',\n",
       "   'hou',\n",
       "   'y t',\n",
       "   'be ',\n",
       "   'o d',\n",
       "   'e o',\n",
       "   'e b',\n",
       "   'dnt',\n",
       "   'rea',\n",
       "   ' it',\n",
       "   'm a',\n",
       "   ' ma',\n",
       "   'ith',\n",
       "   'ted',\n",
       "   ' ho',\n",
       "   'ke ',\n",
       "   ' lo',\n",
       "   'hey',\n",
       "   'out',\n",
       "   'wit',\n",
       "   'd a',\n",
       "   't s',\n",
       "   'kno',\n",
       "   'ey ',\n",
       "   'le ',\n",
       "   'ill',\n",
       "   'snt',\n",
       "   'ne ',\n",
       "   's s',\n",
       "   ' mo',\n",
       "   'e m',\n",
       "   ' al',\n",
       "   ' se',\n",
       "   'ion',\n",
       "   's i',\n",
       "   'd h',\n",
       "   'y w',\n",
       "   'not',\n",
       "   'don',\n",
       "   'it ',\n",
       "   'wan',\n",
       "   'ate',\n",
       "   'se ',\n",
       "   'r t',\n",
       "   'aid',\n",
       "   'ay ',\n",
       "   'y s',\n",
       "   'ht ',\n",
       "   'd M',\n",
       "   'ink',\n",
       "   'm w',\n",
       "   'ad ',\n",
       "   'ear',\n",
       "   ' pr',\n",
       "   't o',\n",
       "   'e f',\n",
       "   'et ',\n",
       "   'ery',\n",
       "   't i',\n",
       "   'she',\n",
       "   'ive',\n",
       "   'He ',\n",
       "   'e p',\n",
       "   's w',\n",
       "   'n a',\n",
       "   ' at',\n",
       "   'sai',\n",
       "   'oin',\n",
       "   'm i',\n",
       "   'ugh',\n",
       "   'ur ',\n",
       "   ' as',\n",
       "   'y a',\n",
       "   'nk ',\n",
       "   'any',\n",
       "   's h']},\n",
       " 'deu': {'50': ['en ',\n",
       "   'ch ',\n",
       "   'ich',\n",
       "   'er ',\n",
       "   'ein',\n",
       "   'ie ',\n",
       "   'cht',\n",
       "   'st ',\n",
       "   'sch',\n",
       "   'in ',\n",
       "   ' de',\n",
       "   'ine',\n",
       "   ' ei',\n",
       "   'ht ',\n",
       "   'te ',\n",
       "   'che',\n",
       "   ' da',\n",
       "   ' di',\n",
       "   'as ',\n",
       "   'ist',\n",
       "   ' ge',\n",
       "   'der',\n",
       "   ' ni',\n",
       "   't d',\n",
       "   ' ha',\n",
       "   'nd ',\n",
       "   'nic',\n",
       "   ' is',\n",
       "   'Ich',\n",
       "   'die',\n",
       "   'den',\n",
       "   'es ',\n",
       "   ' zu',\n",
       "   'gen',\n",
       "   'Tom',\n",
       "   'hen',\n",
       "   ' si',\n",
       "   'n d',\n",
       "   'ne ',\n",
       "   'om ',\n",
       "   'das',\n",
       "   ' un',\n",
       "   ' mi',\n",
       "   'ten',\n",
       "   ' wi',\n",
       "   'und',\n",
       "   'nen',\n",
       "   'ach',\n",
       "   'ter',\n",
       "   ' au'],\n",
       "  '100': ['en ',\n",
       "   'ch ',\n",
       "   'ich',\n",
       "   'er ',\n",
       "   'ein',\n",
       "   'ie ',\n",
       "   'cht',\n",
       "   'st ',\n",
       "   'sch',\n",
       "   'in ',\n",
       "   ' de',\n",
       "   'ine',\n",
       "   ' ei',\n",
       "   'ht ',\n",
       "   'te ',\n",
       "   'che',\n",
       "   ' da',\n",
       "   ' di',\n",
       "   'as ',\n",
       "   'ist',\n",
       "   ' ge',\n",
       "   'der',\n",
       "   ' ni',\n",
       "   't d',\n",
       "   ' ha',\n",
       "   'nd ',\n",
       "   'nic',\n",
       "   ' is',\n",
       "   'Ich',\n",
       "   'die',\n",
       "   'den',\n",
       "   'es ',\n",
       "   ' zu',\n",
       "   'gen',\n",
       "   'Tom',\n",
       "   'hen',\n",
       "   ' si',\n",
       "   'n d',\n",
       "   'ne ',\n",
       "   'om ',\n",
       "   'das',\n",
       "   ' un',\n",
       "   ' mi',\n",
       "   'ten',\n",
       "   ' wi',\n",
       "   'und',\n",
       "   'nen',\n",
       "   'ach',\n",
       "   'ter',\n",
       "   ' au',\n",
       "   'nde',\n",
       "   'ir ',\n",
       "   'zu ',\n",
       "   ' se',\n",
       "   ' be',\n",
       "   'ste',\n",
       "   'ben',\n",
       "   'abe',\n",
       "   'ass',\n",
       "   'it ',\n",
       "   'e d',\n",
       "   ' we',\n",
       "   ' du',\n",
       "   'eit',\n",
       "   't e',\n",
       "   ' wa',\n",
       "   ' in',\n",
       "   'Sie',\n",
       "   ' er',\n",
       "   'ver',\n",
       "   ' ic',\n",
       "   'n s',\n",
       "   'nn ',\n",
       "   'sse',\n",
       "   'em ',\n",
       "   ' sc',\n",
       "   'e i',\n",
       "   'ss ',\n",
       "   'nge',\n",
       "   'auf',\n",
       "   'du ',\n",
       "   'sen',\n",
       "   'n w',\n",
       "   'ren',\n",
       "   'hat',\n",
       "   't s',\n",
       "   'e s',\n",
       "   ' ve',\n",
       "   'r d',\n",
       "   'ber',\n",
       "   'lle',\n",
       "   'hab',\n",
       "   'hr ',\n",
       "   'lic',\n",
       "   'ers',\n",
       "   'ann',\n",
       "   'ung',\n",
       "   'ere',\n",
       "   'ier',\n",
       "   's i'],\n",
       "  '200': ['en ',\n",
       "   'ch ',\n",
       "   'ich',\n",
       "   'er ',\n",
       "   'ein',\n",
       "   'ie ',\n",
       "   'cht',\n",
       "   'st ',\n",
       "   'sch',\n",
       "   'in ',\n",
       "   ' de',\n",
       "   'ine',\n",
       "   ' ei',\n",
       "   'ht ',\n",
       "   'te ',\n",
       "   'che',\n",
       "   ' da',\n",
       "   ' di',\n",
       "   'as ',\n",
       "   'ist',\n",
       "   ' ge',\n",
       "   'der',\n",
       "   ' ni',\n",
       "   't d',\n",
       "   ' ha',\n",
       "   'nd ',\n",
       "   'nic',\n",
       "   ' is',\n",
       "   'Ich',\n",
       "   'die',\n",
       "   'den',\n",
       "   'es ',\n",
       "   ' zu',\n",
       "   'gen',\n",
       "   'Tom',\n",
       "   'hen',\n",
       "   ' si',\n",
       "   'n d',\n",
       "   'ne ',\n",
       "   'om ',\n",
       "   'das',\n",
       "   ' un',\n",
       "   ' mi',\n",
       "   'ten',\n",
       "   ' wi',\n",
       "   'und',\n",
       "   'nen',\n",
       "   'ach',\n",
       "   'ter',\n",
       "   ' au',\n",
       "   'nde',\n",
       "   'ir ',\n",
       "   'zu ',\n",
       "   ' se',\n",
       "   ' be',\n",
       "   'ste',\n",
       "   'ben',\n",
       "   'abe',\n",
       "   'ass',\n",
       "   'it ',\n",
       "   'e d',\n",
       "   ' we',\n",
       "   ' du',\n",
       "   'eit',\n",
       "   't e',\n",
       "   ' wa',\n",
       "   ' in',\n",
       "   'Sie',\n",
       "   ' er',\n",
       "   'ver',\n",
       "   ' ic',\n",
       "   'n s',\n",
       "   'nn ',\n",
       "   'sse',\n",
       "   'em ',\n",
       "   ' sc',\n",
       "   'e i',\n",
       "   'ss ',\n",
       "   'nge',\n",
       "   'auf',\n",
       "   'du ',\n",
       "   'sen',\n",
       "   'n w',\n",
       "   'ren',\n",
       "   'hat',\n",
       "   't s',\n",
       "   'e s',\n",
       "   ' ve',\n",
       "   'r d',\n",
       "   'ber',\n",
       "   'lle',\n",
       "   'hab',\n",
       "   'hr ',\n",
       "   'lic',\n",
       "   'ers',\n",
       "   'ann',\n",
       "   'ung',\n",
       "   'ere',\n",
       "   'ier',\n",
       "   's i',\n",
       "   'mme',\n",
       "   'on ',\n",
       "   ' al',\n",
       "   'sei',\n",
       "   'uch',\n",
       "   'ern',\n",
       "   ' vo',\n",
       "   'aus',\n",
       "   'tte',\n",
       "   ' es',\n",
       "   ' ih',\n",
       "   'n S',\n",
       "   'ind',\n",
       "   'and',\n",
       "   'n i',\n",
       "   'hre',\n",
       "   ' an',\n",
       "   'ese',\n",
       "   'an ',\n",
       "   'r s',\n",
       "   'rde',\n",
       "   'at ',\n",
       "   'sic',\n",
       "   ' so',\n",
       "   'e e',\n",
       "   'de ',\n",
       "   'sie',\n",
       "   't i',\n",
       "   'n e',\n",
       "   'war',\n",
       "   ' ma',\n",
       "   'wir',\n",
       "   ' me',\n",
       "   'he ',\n",
       "   'Die',\n",
       "   'ges',\n",
       "   'mit',\n",
       "   'nte',\n",
       "   'um ',\n",
       "   't m',\n",
       "   'be ',\n",
       "   'och',\n",
       "   'est',\n",
       "   'lte',\n",
       "   'r e',\n",
       "   ' To',\n",
       "   'rei',\n",
       "   'hte',\n",
       "   'ehe',\n",
       "   'Sch',\n",
       "   's d',\n",
       "   'men',\n",
       "   'ies',\n",
       "   'enn',\n",
       "   ' Ma',\n",
       "   'h d',\n",
       "   'n a',\n",
       "   'e m',\n",
       "   'ert',\n",
       "   'ehr',\n",
       "   't w',\n",
       "   'wei',\n",
       "   'uf ',\n",
       "   't a',\n",
       "   ' Ge',\n",
       "   'h h',\n",
       "   'lei',\n",
       "   'ng ',\n",
       "   ' Sc',\n",
       "   ' ka',\n",
       "   'iel',\n",
       "   'h w',\n",
       "   ' bi',\n",
       "   're ',\n",
       "   'ebe',\n",
       "   'ige',\n",
       "   'ge ',\n",
       "   'Mar',\n",
       "   'ner',\n",
       "   'isc',\n",
       "   'all',\n",
       "   'ar ',\n",
       "   ' Si',\n",
       "   'n m',\n",
       "   'e S',\n",
       "   'end',\n",
       "   'r i',\n",
       "   't n',\n",
       "   'len',\n",
       "   'ari',\n",
       "   ' im',\n",
       "   'Er ',\n",
       "   'ger',\n",
       "   'was',\n",
       "   'hei',\n",
       "   'e n',\n",
       "   'ang',\n",
       "   'mei',\n",
       "   'n g',\n",
       "   'Das']},\n",
       " 'spa': {'50': [' de',\n",
       "   ' es',\n",
       "   'de ',\n",
       "   'os ',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'es ',\n",
       "   'ue ',\n",
       "   ' la',\n",
       "   'la ',\n",
       "   ' co',\n",
       "   'as ',\n",
       "   'do ',\n",
       "   'est',\n",
       "   ' en',\n",
       "   'en ',\n",
       "   ' un',\n",
       "   'el ',\n",
       "   ' a ',\n",
       "   'ent',\n",
       "   ' el',\n",
       "   'te ',\n",
       "   'no ',\n",
       "   ' no',\n",
       "   'o e',\n",
       "   'nte',\n",
       "   ' ha',\n",
       "   ' se',\n",
       "   'ra ',\n",
       "   'e e',\n",
       "   'a e',\n",
       "   'sta',\n",
       "   'ien',\n",
       "   'con',\n",
       "   ' pa',\n",
       "   'e l',\n",
       "   'na ',\n",
       "   'ar ',\n",
       "   'a c',\n",
       "   ' lo',\n",
       "   's d',\n",
       "   'ado',\n",
       "   ' ca',\n",
       "   'or ',\n",
       "   'o d',\n",
       "   'a d',\n",
       "   'se ',\n",
       "   ' po',\n",
       "   'un ',\n",
       "   'on '],\n",
       "  '100': [' de',\n",
       "   ' es',\n",
       "   'de ',\n",
       "   'os ',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'es ',\n",
       "   'ue ',\n",
       "   ' la',\n",
       "   'la ',\n",
       "   ' co',\n",
       "   'as ',\n",
       "   'do ',\n",
       "   'est',\n",
       "   ' en',\n",
       "   'en ',\n",
       "   ' un',\n",
       "   'el ',\n",
       "   ' a ',\n",
       "   'ent',\n",
       "   ' el',\n",
       "   'te ',\n",
       "   'no ',\n",
       "   ' no',\n",
       "   'o e',\n",
       "   'nte',\n",
       "   ' ha',\n",
       "   ' se',\n",
       "   'ra ',\n",
       "   'e e',\n",
       "   'a e',\n",
       "   'sta',\n",
       "   'ien',\n",
       "   'con',\n",
       "   ' pa',\n",
       "   'e l',\n",
       "   'na ',\n",
       "   'ar ',\n",
       "   'a c',\n",
       "   ' lo',\n",
       "   's d',\n",
       "   'ado',\n",
       "   ' ca',\n",
       "   'or ',\n",
       "   'o d',\n",
       "   'a d',\n",
       "   'se ',\n",
       "   ' po',\n",
       "   'un ',\n",
       "   'on ',\n",
       "   'Tom',\n",
       "   'ta ',\n",
       "   ' me',\n",
       "   'lo ',\n",
       "   'a l',\n",
       "   'a p',\n",
       "   's e',\n",
       "   'ndo',\n",
       "   'ía ',\n",
       "   'e d',\n",
       "   'ro ',\n",
       "   'n e',\n",
       "   'er ',\n",
       "   'o q',\n",
       "   'e a',\n",
       "   ' pe',\n",
       "   'a m',\n",
       "   ' al',\n",
       "   'par',\n",
       "   'to ',\n",
       "   'una',\n",
       "   'o p',\n",
       "   ' te',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'res',\n",
       "   'tra',\n",
       "   'los',\n",
       "   'om ',\n",
       "   'o s',\n",
       "   'ero',\n",
       "   'o a',\n",
       "   'e c',\n",
       "   's p',\n",
       "   ' pr',\n",
       "   ' di',\n",
       "   ' mu',\n",
       "   's a',\n",
       "   'por',\n",
       "   'a a',\n",
       "   'e s',\n",
       "   'ara',\n",
       "   'No ',\n",
       "   'a s',\n",
       "   'per',\n",
       "   'era',\n",
       "   'ier',\n",
       "   'me ',\n",
       "   'an ',\n",
       "   ' mi'],\n",
       "  '200': [' de',\n",
       "   ' es',\n",
       "   'de ',\n",
       "   'os ',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'es ',\n",
       "   'ue ',\n",
       "   ' la',\n",
       "   'la ',\n",
       "   ' co',\n",
       "   'as ',\n",
       "   'do ',\n",
       "   'est',\n",
       "   ' en',\n",
       "   'en ',\n",
       "   ' un',\n",
       "   'el ',\n",
       "   ' a ',\n",
       "   'ent',\n",
       "   ' el',\n",
       "   'te ',\n",
       "   'no ',\n",
       "   ' no',\n",
       "   'o e',\n",
       "   'nte',\n",
       "   ' ha',\n",
       "   ' se',\n",
       "   'ra ',\n",
       "   'e e',\n",
       "   'a e',\n",
       "   'sta',\n",
       "   'ien',\n",
       "   'con',\n",
       "   ' pa',\n",
       "   'e l',\n",
       "   'na ',\n",
       "   'ar ',\n",
       "   'a c',\n",
       "   ' lo',\n",
       "   's d',\n",
       "   'ado',\n",
       "   ' ca',\n",
       "   'or ',\n",
       "   'o d',\n",
       "   'a d',\n",
       "   'se ',\n",
       "   ' po',\n",
       "   'un ',\n",
       "   'on ',\n",
       "   'Tom',\n",
       "   'ta ',\n",
       "   ' me',\n",
       "   'lo ',\n",
       "   'a l',\n",
       "   'a p',\n",
       "   's e',\n",
       "   'ndo',\n",
       "   'ía ',\n",
       "   'e d',\n",
       "   'ro ',\n",
       "   'n e',\n",
       "   'er ',\n",
       "   'o q',\n",
       "   'e a',\n",
       "   ' pe',\n",
       "   'a m',\n",
       "   ' al',\n",
       "   'par',\n",
       "   'to ',\n",
       "   'una',\n",
       "   'o p',\n",
       "   ' te',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'res',\n",
       "   'tra',\n",
       "   'los',\n",
       "   'om ',\n",
       "   'o s',\n",
       "   'ero',\n",
       "   'o a',\n",
       "   'e c',\n",
       "   's p',\n",
       "   ' pr',\n",
       "   ' di',\n",
       "   ' mu',\n",
       "   's a',\n",
       "   'por',\n",
       "   'a a',\n",
       "   'e s',\n",
       "   'ara',\n",
       "   'No ',\n",
       "   'a s',\n",
       "   'per',\n",
       "   'era',\n",
       "   'ier',\n",
       "   'me ',\n",
       "   'an ',\n",
       "   ' mi',\n",
       "   'com',\n",
       "   'men',\n",
       "   ' ve',\n",
       "   'ant',\n",
       "   ' re',\n",
       "   ' y ',\n",
       "   ' ma',\n",
       "   'da ',\n",
       "   ' so',\n",
       "   's c',\n",
       "   'n l',\n",
       "   'mos',\n",
       "   'lla',\n",
       "   'aba',\n",
       "   ' si',\n",
       "   'las',\n",
       "   'stá',\n",
       "   'e t',\n",
       "   'and',\n",
       "   'e m',\n",
       "   'ene',\n",
       "   'o t',\n",
       "   'o c',\n",
       "   'ste',\n",
       "   'al ',\n",
       "   'e h',\n",
       "   'nto',\n",
       "   'o l',\n",
       "   ' le',\n",
       "   ' ti',\n",
       "   're ',\n",
       "   'a t',\n",
       "   'tar',\n",
       "   'ría',\n",
       "   'ás ',\n",
       "   'nta',\n",
       "   's l',\n",
       "   's s',\n",
       "   's m',\n",
       "   'ada',\n",
       "   ' to',\n",
       "   ' pu',\n",
       "   'pre',\n",
       "   'ión',\n",
       "   'end',\n",
       "   ' vi',\n",
       "   'El ',\n",
       "   'o m',\n",
       "   'sto',\n",
       "   'des',\n",
       "   'ten',\n",
       "   'tie',\n",
       "   'ida',\n",
       "   'ued',\n",
       "   'r e',\n",
       "   ' in',\n",
       "   'le ',\n",
       "   ' tr',\n",
       "   'e q',\n",
       "   'ido',\n",
       "   'dos',\n",
       "   'a v',\n",
       "   ' cu',\n",
       "   'dad',\n",
       "   'ran',\n",
       "   'Est',\n",
       "   'ón ',\n",
       "   'uer',\n",
       "   'aci',\n",
       "   'más',\n",
       "   'qui',\n",
       "   'a n',\n",
       "   'mo ',\n",
       "   'pue',\n",
       "   'go ',\n",
       "   'r a',\n",
       "   'ció',\n",
       "   ' sa',\n",
       "   'tan',\n",
       "   'uie',\n",
       "   'e n',\n",
       "   'hab',\n",
       "   'La ',\n",
       "   'tá ',\n",
       "   'ver',\n",
       "   'ace',\n",
       "   'o h',\n",
       "   'n a',\n",
       "   'n p',\n",
       "   'ué ',\n",
       "   'rec',\n",
       "   'n c',\n",
       "   'su ',\n",
       "   'n d',\n",
       "   'cer',\n",
       "   ' ta',\n",
       "   's t',\n",
       "   'nos',\n",
       "   ' má',\n",
       "   'ist']},\n",
       " 'fra': {'50': [' de',\n",
       "   'es ',\n",
       "   'de ',\n",
       "   'le ',\n",
       "   ' pa',\n",
       "   'ent',\n",
       "   ' qu',\n",
       "   'nt ',\n",
       "   'ne ',\n",
       "   're ',\n",
       "   'us ',\n",
       "   ' le',\n",
       "   'que',\n",
       "   'est',\n",
       "   'e p',\n",
       "   'ous',\n",
       "   ' la',\n",
       "   'is ',\n",
       "   'e d',\n",
       "   'ue ',\n",
       "   'e s',\n",
       "   'st ',\n",
       "   'pas',\n",
       "   'lle',\n",
       "   'as ',\n",
       "   'ais',\n",
       "   'e l',\n",
       "   'it ',\n",
       "   'la ',\n",
       "   ' un',\n",
       "   's d',\n",
       "   'er ',\n",
       "   'Je ',\n",
       "   'e c',\n",
       "   'our',\n",
       "   'on ',\n",
       "   'ns ',\n",
       "   's p',\n",
       "   ' ne',\n",
       "   ' à ',\n",
       "   ' co',\n",
       "   ' es',\n",
       "   'te ',\n",
       "   'e n',\n",
       "   ' ma',\n",
       "   'les',\n",
       "   'ait',\n",
       "   ' vo',\n",
       "   ' so',\n",
       "   'e t'],\n",
       "  '100': [' de',\n",
       "   'es ',\n",
       "   'de ',\n",
       "   'le ',\n",
       "   ' pa',\n",
       "   'ent',\n",
       "   ' qu',\n",
       "   'nt ',\n",
       "   'ne ',\n",
       "   're ',\n",
       "   'us ',\n",
       "   ' le',\n",
       "   'que',\n",
       "   'est',\n",
       "   'e p',\n",
       "   'ous',\n",
       "   ' la',\n",
       "   'is ',\n",
       "   'e d',\n",
       "   'ue ',\n",
       "   'e s',\n",
       "   'st ',\n",
       "   'pas',\n",
       "   'lle',\n",
       "   'as ',\n",
       "   'ais',\n",
       "   'e l',\n",
       "   'it ',\n",
       "   'la ',\n",
       "   ' un',\n",
       "   's d',\n",
       "   'er ',\n",
       "   'Je ',\n",
       "   'e c',\n",
       "   'our',\n",
       "   'on ',\n",
       "   'ns ',\n",
       "   's p',\n",
       "   ' ne',\n",
       "   ' à ',\n",
       "   ' co',\n",
       "   ' es',\n",
       "   'te ',\n",
       "   'e n',\n",
       "   ' ma',\n",
       "   'les',\n",
       "   'ait',\n",
       "   ' vo',\n",
       "   ' so',\n",
       "   'e t',\n",
       "   ' en',\n",
       "   'ur ',\n",
       "   'e m',\n",
       "   ' po',\n",
       "   ' ce',\n",
       "   'en ',\n",
       "   'men',\n",
       "   ' se',\n",
       "   's l',\n",
       "   'e v',\n",
       "   'un ',\n",
       "   's a',\n",
       "   'vou',\n",
       "   'tre',\n",
       "   ' fa',\n",
       "   'ce ',\n",
       "   'e q',\n",
       "   ' pe',\n",
       "   'Il ',\n",
       "   'me ',\n",
       "   'son',\n",
       "   't d',\n",
       "   'ont',\n",
       "   ' pr',\n",
       "   'ien',\n",
       "   'ire',\n",
       "   ' a ',\n",
       "   'ant',\n",
       "   'se ',\n",
       "   's s',\n",
       "   't p',\n",
       "   'une',\n",
       "   ' to',\n",
       "   ' tr',\n",
       "   ' ch',\n",
       "   'rai',\n",
       "   ' mo',\n",
       "   'par',\n",
       "   ' re',\n",
       "   ' su',\n",
       "   'e e',\n",
       "   'ai ',\n",
       "   's c',\n",
       "   'eur',\n",
       "   'ons',\n",
       "   'ion',\n",
       "   'fai',\n",
       "   ' da',\n",
       "   't l',\n",
       "   'pou'],\n",
       "  '200': [' de',\n",
       "   'es ',\n",
       "   'de ',\n",
       "   'le ',\n",
       "   ' pa',\n",
       "   'ent',\n",
       "   ' qu',\n",
       "   'nt ',\n",
       "   'ne ',\n",
       "   're ',\n",
       "   'us ',\n",
       "   ' le',\n",
       "   'que',\n",
       "   'est',\n",
       "   'e p',\n",
       "   'ous',\n",
       "   ' la',\n",
       "   'is ',\n",
       "   'e d',\n",
       "   'ue ',\n",
       "   'e s',\n",
       "   'st ',\n",
       "   'pas',\n",
       "   'lle',\n",
       "   'as ',\n",
       "   'ais',\n",
       "   'e l',\n",
       "   'it ',\n",
       "   'la ',\n",
       "   ' un',\n",
       "   's d',\n",
       "   'er ',\n",
       "   'Je ',\n",
       "   'e c',\n",
       "   'our',\n",
       "   'on ',\n",
       "   'ns ',\n",
       "   's p',\n",
       "   ' ne',\n",
       "   ' à ',\n",
       "   ' co',\n",
       "   ' es',\n",
       "   'te ',\n",
       "   'e n',\n",
       "   ' ma',\n",
       "   'les',\n",
       "   'ait',\n",
       "   ' vo',\n",
       "   ' so',\n",
       "   'e t',\n",
       "   ' en',\n",
       "   'ur ',\n",
       "   'e m',\n",
       "   ' po',\n",
       "   ' ce',\n",
       "   'en ',\n",
       "   'men',\n",
       "   ' se',\n",
       "   's l',\n",
       "   'e v',\n",
       "   'un ',\n",
       "   's a',\n",
       "   'vou',\n",
       "   'tre',\n",
       "   ' fa',\n",
       "   'ce ',\n",
       "   'e q',\n",
       "   ' pe',\n",
       "   'Il ',\n",
       "   'me ',\n",
       "   'son',\n",
       "   't d',\n",
       "   'ont',\n",
       "   ' pr',\n",
       "   'ien',\n",
       "   'ire',\n",
       "   ' a ',\n",
       "   'ant',\n",
       "   'se ',\n",
       "   's s',\n",
       "   't p',\n",
       "   'une',\n",
       "   ' to',\n",
       "   ' tr',\n",
       "   ' ch',\n",
       "   'rai',\n",
       "   ' mo',\n",
       "   'par',\n",
       "   ' re',\n",
       "   ' su',\n",
       "   'e e',\n",
       "   'ai ',\n",
       "   's c',\n",
       "   'eur',\n",
       "   'ons',\n",
       "   'ion',\n",
       "   'fai',\n",
       "   ' da',\n",
       "   't l',\n",
       "   'pou',\n",
       "   'e a',\n",
       "   ' au',\n",
       "   'eux',\n",
       "   ' no',\n",
       "   ' sa',\n",
       "   'il ',\n",
       "   'e f',\n",
       "   ' av',\n",
       "   ' pl',\n",
       "   ' me',\n",
       "   'ans',\n",
       "   's e',\n",
       "   'ux ',\n",
       "   'mme',\n",
       "   ' di',\n",
       "   'mai',\n",
       "   'des',\n",
       "   ' te',\n",
       "   'res',\n",
       "   'Tom',\n",
       "   'tu ',\n",
       "   'tou',\n",
       "   'con',\n",
       "   'ell',\n",
       "   'ez ',\n",
       "   'et ',\n",
       "   'end',\n",
       "   'r l',\n",
       "   'qui',\n",
       "   'omm',\n",
       "   'a p',\n",
       "   'onn',\n",
       "   'che',\n",
       "   'n d',\n",
       "   'ut ',\n",
       "   'om ',\n",
       "   'tai',\n",
       "   'ill',\n",
       "   'nte',\n",
       "   'ui ',\n",
       "   'e r',\n",
       "   'tes',\n",
       "   'air',\n",
       "   ' je',\n",
       "   'ren',\n",
       "   'dan',\n",
       "   'out',\n",
       "   'sse',\n",
       "   'ain',\n",
       "   ' dé',\n",
       "   ' du',\n",
       "   ' ét',\n",
       "   'vai',\n",
       "   'uis',\n",
       "   'je ',\n",
       "   'ouv',\n",
       "   'ir ',\n",
       "   'eme',\n",
       "   'com',\n",
       "   ' vi',\n",
       "   'ens',\n",
       "   'ure',\n",
       "   's m',\n",
       "   's t',\n",
       "   'oir',\n",
       "   'ter',\n",
       "   'ère',\n",
       "   'tte',\n",
       "   ' et',\n",
       "   'Ell',\n",
       "   'ave',\n",
       "   'tio',\n",
       "   'e j',\n",
       "   'n p',\n",
       "   'ois',\n",
       "   'nou',\n",
       "   'rie',\n",
       "   'cha',\n",
       "   's q',\n",
       "   'té ',\n",
       "   'ort',\n",
       "   'tra',\n",
       "   'r d',\n",
       "   's v',\n",
       "   't a',\n",
       "   's n',\n",
       "   'ava',\n",
       "   'nne',\n",
       "   ' na',\n",
       "   'lai',\n",
       "   'uel',\n",
       "   'rs ',\n",
       "   'au ',\n",
       "   'du ',\n",
       "   'ie ',\n",
       "   'and',\n",
       "   'ses',\n",
       "   'cou',\n",
       "   ' ve',\n",
       "   ' do']},\n",
       " 'por': {'50': [' de',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'de ',\n",
       "   'ão ',\n",
       "   ' co',\n",
       "   'ue ',\n",
       "   ' es',\n",
       "   'os ',\n",
       "   'do ',\n",
       "   'om ',\n",
       "   'est',\n",
       "   ' se',\n",
       "   'as ',\n",
       "   'em ',\n",
       "   ' o ',\n",
       "   ' a ',\n",
       "   ' um',\n",
       "   'ent',\n",
       "   'nte',\n",
       "   'ar ',\n",
       "   'ra ',\n",
       "   'Tom',\n",
       "   ' pa',\n",
       "   'não',\n",
       "   ' nã',\n",
       "   'o d',\n",
       "   'com',\n",
       "   'ocê',\n",
       "   'er ',\n",
       "   'to ',\n",
       "   'e e',\n",
       "   'te ',\n",
       "   ' me',\n",
       "   ' te',\n",
       "   'ou ',\n",
       "   ' po',\n",
       "   ' é ',\n",
       "   'a d',\n",
       "   'eu ',\n",
       "   'sta',\n",
       "   'ia ',\n",
       "   'Eu ',\n",
       "   'par',\n",
       "   'da ',\n",
       "   ' ca',\n",
       "   'ara',\n",
       "   'cê ',\n",
       "   'se ',\n",
       "   'a c'],\n",
       "  '100': [' de',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'de ',\n",
       "   'ão ',\n",
       "   ' co',\n",
       "   'ue ',\n",
       "   ' es',\n",
       "   'os ',\n",
       "   'do ',\n",
       "   'om ',\n",
       "   'est',\n",
       "   ' se',\n",
       "   'as ',\n",
       "   'em ',\n",
       "   ' o ',\n",
       "   ' a ',\n",
       "   ' um',\n",
       "   'ent',\n",
       "   'nte',\n",
       "   'ar ',\n",
       "   'ra ',\n",
       "   'Tom',\n",
       "   ' pa',\n",
       "   'não',\n",
       "   ' nã',\n",
       "   'o d',\n",
       "   'com',\n",
       "   'ocê',\n",
       "   'er ',\n",
       "   'to ',\n",
       "   'e e',\n",
       "   'te ',\n",
       "   ' me',\n",
       "   ' te',\n",
       "   'ou ',\n",
       "   ' po',\n",
       "   ' é ',\n",
       "   'a d',\n",
       "   'eu ',\n",
       "   'sta',\n",
       "   'ia ',\n",
       "   'Eu ',\n",
       "   'par',\n",
       "   'da ',\n",
       "   ' ca',\n",
       "   'ara',\n",
       "   'cê ',\n",
       "   'se ',\n",
       "   'a c',\n",
       "   ' pr',\n",
       "   'o p',\n",
       "   'ndo',\n",
       "   'o e',\n",
       "   'e a',\n",
       "   'ma ',\n",
       "   ' fa',\n",
       "   'um ',\n",
       "   'con',\n",
       "   'o q',\n",
       "   ' ma',\n",
       "   'sso',\n",
       "   ' vo',\n",
       "   'or ',\n",
       "   'ado',\n",
       "   'a e',\n",
       "   ' pe',\n",
       "   ' di',\n",
       "   'o a',\n",
       "   's d',\n",
       "   'o c',\n",
       "   'es ',\n",
       "   'e d',\n",
       "   ' do',\n",
       "   ' no',\n",
       "   'uma',\n",
       "   'a m',\n",
       "   'a p',\n",
       "   'ria',\n",
       "   'men',\n",
       "   'o m',\n",
       "   'o s',\n",
       "   'a a',\n",
       "   'is ',\n",
       "   'pre',\n",
       "   ' em',\n",
       "   'and',\n",
       "   'le ',\n",
       "   'voc',\n",
       "   'ta ',\n",
       "   'e v',\n",
       "   'ro ',\n",
       "   'e c',\n",
       "   'ant',\n",
       "   'so ',\n",
       "   'ito',\n",
       "   'la ',\n",
       "   ' To',\n",
       "   's e',\n",
       "   ' mu'],\n",
       "  '200': [' de',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'de ',\n",
       "   'ão ',\n",
       "   ' co',\n",
       "   'ue ',\n",
       "   ' es',\n",
       "   'os ',\n",
       "   'do ',\n",
       "   'om ',\n",
       "   'est',\n",
       "   ' se',\n",
       "   'as ',\n",
       "   'em ',\n",
       "   ' o ',\n",
       "   ' a ',\n",
       "   ' um',\n",
       "   'ent',\n",
       "   'nte',\n",
       "   'ar ',\n",
       "   'ra ',\n",
       "   'Tom',\n",
       "   ' pa',\n",
       "   'não',\n",
       "   ' nã',\n",
       "   'o d',\n",
       "   'com',\n",
       "   'ocê',\n",
       "   'er ',\n",
       "   'to ',\n",
       "   'e e',\n",
       "   'te ',\n",
       "   ' me',\n",
       "   ' te',\n",
       "   'ou ',\n",
       "   ' po',\n",
       "   ' é ',\n",
       "   'a d',\n",
       "   'eu ',\n",
       "   'sta',\n",
       "   'ia ',\n",
       "   'Eu ',\n",
       "   'par',\n",
       "   'da ',\n",
       "   ' ca',\n",
       "   'ara',\n",
       "   'cê ',\n",
       "   'se ',\n",
       "   'a c',\n",
       "   ' pr',\n",
       "   'o p',\n",
       "   'ndo',\n",
       "   'o e',\n",
       "   'e a',\n",
       "   'ma ',\n",
       "   ' fa',\n",
       "   'um ',\n",
       "   'con',\n",
       "   'o q',\n",
       "   ' ma',\n",
       "   'sso',\n",
       "   ' vo',\n",
       "   'or ',\n",
       "   'ado',\n",
       "   'a e',\n",
       "   ' pe',\n",
       "   ' di',\n",
       "   'o a',\n",
       "   's d',\n",
       "   'o c',\n",
       "   'es ',\n",
       "   'e d',\n",
       "   ' do',\n",
       "   ' no',\n",
       "   'uma',\n",
       "   'a m',\n",
       "   'a p',\n",
       "   'ria',\n",
       "   'men',\n",
       "   'o m',\n",
       "   'o s',\n",
       "   'a a',\n",
       "   'is ',\n",
       "   'pre',\n",
       "   ' em',\n",
       "   'and',\n",
       "   'le ',\n",
       "   'voc',\n",
       "   'ta ',\n",
       "   'e v',\n",
       "   'ro ',\n",
       "   'e c',\n",
       "   'ant',\n",
       "   'so ',\n",
       "   'ito',\n",
       "   'la ',\n",
       "   ' To',\n",
       "   's e',\n",
       "   ' mu',\n",
       "   's p',\n",
       "   'e p',\n",
       "   ' e ',\n",
       "   'res',\n",
       "   'inh',\n",
       "   'iss',\n",
       "   'ais',\n",
       "   'nto',\n",
       "   'Voc',\n",
       "   'mos',\n",
       "   'o t',\n",
       "   'tem',\n",
       "   'stá',\n",
       "   'ele',\n",
       "   'ess',\n",
       "   'a s',\n",
       "   ' na',\n",
       "   'me ',\n",
       "   's a',\n",
       "   'e s',\n",
       "   'tra',\n",
       "   ' da',\n",
       "   'ver',\n",
       "   'nha',\n",
       "   'tá ',\n",
       "   'am ',\n",
       "   'por',\n",
       "   'ora',\n",
       "   'e o',\n",
       "   'ont',\n",
       "   'sto',\n",
       "   'per',\n",
       "   ' re',\n",
       "   ' ve',\n",
       "   'tar',\n",
       "   'm e',\n",
       "   'no ',\n",
       "   ' el',\n",
       "   'e n',\n",
       "   'mai',\n",
       "   'a n',\n",
       "   'e t',\n",
       "   'sa ',\n",
       "   'sse',\n",
       "   'ida',\n",
       "   's c',\n",
       "   'e m',\n",
       "   'm a',\n",
       "   'na ',\n",
       "   'ava',\n",
       "   'r a',\n",
       "   ' fo',\n",
       "   'nho',\n",
       "   'ada',\n",
       "   'ho ',\n",
       "   's s',\n",
       "   'o n',\n",
       "   'e f',\n",
       "   'ha ',\n",
       "   'nta',\n",
       "   'Ele',\n",
       "   'uit',\n",
       "   'ei ',\n",
       "   'a f',\n",
       "   'ost',\n",
       "   'uer',\n",
       "   'rec',\n",
       "   'dos',\n",
       "   'end',\n",
       "   'nde',\n",
       "   'ica',\n",
       "   ' en',\n",
       "   ' sa',\n",
       "   'o f',\n",
       "   ' vi',\n",
       "   ' so',\n",
       "   'mui',\n",
       "   'Não',\n",
       "   ' ac',\n",
       "   ' in',\n",
       "   'ran',\n",
       "   'm p',\n",
       "   'ela',\n",
       "   'va ',\n",
       "   'ter',\n",
       "   'ade',\n",
       "   ' tr',\n",
       "   ' fi',\n",
       "   's n',\n",
       "   'm c',\n",
       "   'a t',\n",
       "   'car',\n",
       "   'era',\n",
       "   'u a',\n",
       "   'u n',\n",
       "   'des',\n",
       "   'faz',\n",
       "   ' is',\n",
       "   'm d',\n",
       "   'a v']},\n",
       " 'ita': {'50': ['re ',\n",
       "   'to ',\n",
       "   'on ',\n",
       "   ' co',\n",
       "   'no ',\n",
       "   ' di',\n",
       "   ' in',\n",
       "   ' a ',\n",
       "   'che',\n",
       "   'he ',\n",
       "   'are',\n",
       "   ' ch',\n",
       "   'di ',\n",
       "   'te ',\n",
       "   'o a',\n",
       "   'a c',\n",
       "   'la ',\n",
       "   'in ',\n",
       "   ' un',\n",
       "   ' no',\n",
       "   'o c',\n",
       "   'Tom',\n",
       "   ' la',\n",
       "   'o d',\n",
       "   'ent',\n",
       "   'and',\n",
       "   'Non',\n",
       "   ' pe',\n",
       "   'ato',\n",
       "   'e a',\n",
       "   'per',\n",
       "   'str',\n",
       "   ' è ',\n",
       "   ' qu',\n",
       "   'om ',\n",
       "   ' de',\n",
       "   'ono',\n",
       "   'non',\n",
       "   'cos',\n",
       "   'ost',\n",
       "   ' so',\n",
       "   'a s',\n",
       "   'sta',\n",
       "   'ta ',\n",
       "   'i s',\n",
       "   'e i',\n",
       "   'est',\n",
       "   'o s',\n",
       "   ' st',\n",
       "   'a d'],\n",
       "  '100': ['re ',\n",
       "   'to ',\n",
       "   'on ',\n",
       "   ' co',\n",
       "   'no ',\n",
       "   ' di',\n",
       "   ' in',\n",
       "   ' a ',\n",
       "   'che',\n",
       "   'he ',\n",
       "   'are',\n",
       "   ' ch',\n",
       "   'di ',\n",
       "   'te ',\n",
       "   'o a',\n",
       "   'a c',\n",
       "   'la ',\n",
       "   'in ',\n",
       "   ' un',\n",
       "   ' no',\n",
       "   'o c',\n",
       "   'Tom',\n",
       "   ' la',\n",
       "   'o d',\n",
       "   'ent',\n",
       "   'and',\n",
       "   'Non',\n",
       "   ' pe',\n",
       "   'ato',\n",
       "   'e a',\n",
       "   'per',\n",
       "   'str',\n",
       "   ' è ',\n",
       "   ' qu',\n",
       "   'om ',\n",
       "   ' de',\n",
       "   'ono',\n",
       "   'non',\n",
       "   'cos',\n",
       "   'ost',\n",
       "   ' so',\n",
       "   'a s',\n",
       "   'sta',\n",
       "   'ta ',\n",
       "   'i s',\n",
       "   'e i',\n",
       "   'est',\n",
       "   'o s',\n",
       "   ' st',\n",
       "   'a d',\n",
       "   'o i',\n",
       "   'le ',\n",
       "   'o p',\n",
       "   'e s',\n",
       "   ' fa',\n",
       "   'ere',\n",
       "   'e d',\n",
       "   'i a',\n",
       "   'nte',\n",
       "   'mo ',\n",
       "   ' an',\n",
       "   'il ',\n",
       "   'con',\n",
       "   ' il',\n",
       "   'e c',\n",
       "   'ti ',\n",
       "   ' pr',\n",
       "   'ell',\n",
       "   'ro ',\n",
       "   'ra ',\n",
       "   'ire',\n",
       "   'er ',\n",
       "   'a p',\n",
       "   'tto',\n",
       "   'na ',\n",
       "   ' ha',\n",
       "   'i c',\n",
       "   'un ',\n",
       "   'sto',\n",
       "   ' mi',\n",
       "   'ett',\n",
       "   ' se',\n",
       "   ' le',\n",
       "   'do ',\n",
       "   'gli',\n",
       "   'e l',\n",
       "   ' ma',\n",
       "   'ion',\n",
       "   ' pa',\n",
       "   'iam',\n",
       "   'amo',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'ess',\n",
       "   'era',\n",
       "   'lla',\n",
       "   'i p',\n",
       "   'io ',\n",
       "   'o l',\n",
       "   'e n'],\n",
       "  '200': ['re ',\n",
       "   'to ',\n",
       "   'on ',\n",
       "   ' co',\n",
       "   'no ',\n",
       "   ' di',\n",
       "   ' in',\n",
       "   ' a ',\n",
       "   'che',\n",
       "   'he ',\n",
       "   'are',\n",
       "   ' ch',\n",
       "   'di ',\n",
       "   'te ',\n",
       "   'o a',\n",
       "   'a c',\n",
       "   'la ',\n",
       "   'in ',\n",
       "   ' un',\n",
       "   ' no',\n",
       "   'o c',\n",
       "   'Tom',\n",
       "   ' la',\n",
       "   'o d',\n",
       "   'ent',\n",
       "   'and',\n",
       "   'Non',\n",
       "   ' pe',\n",
       "   'ato',\n",
       "   'e a',\n",
       "   'per',\n",
       "   'str',\n",
       "   ' è ',\n",
       "   ' qu',\n",
       "   'om ',\n",
       "   ' de',\n",
       "   'ono',\n",
       "   'non',\n",
       "   'cos',\n",
       "   'ost',\n",
       "   ' so',\n",
       "   'a s',\n",
       "   'sta',\n",
       "   'ta ',\n",
       "   'i s',\n",
       "   'e i',\n",
       "   'est',\n",
       "   'o s',\n",
       "   ' st',\n",
       "   'a d',\n",
       "   'o i',\n",
       "   'le ',\n",
       "   'o p',\n",
       "   'e s',\n",
       "   ' fa',\n",
       "   'ere',\n",
       "   'e d',\n",
       "   'i a',\n",
       "   'nte',\n",
       "   'mo ',\n",
       "   ' an',\n",
       "   'il ',\n",
       "   'con',\n",
       "   ' il',\n",
       "   'e c',\n",
       "   'ti ',\n",
       "   ' pr',\n",
       "   'ell',\n",
       "   'ro ',\n",
       "   'ra ',\n",
       "   'ire',\n",
       "   'er ',\n",
       "   'a p',\n",
       "   'tto',\n",
       "   'na ',\n",
       "   ' ha',\n",
       "   'i c',\n",
       "   'un ',\n",
       "   'sto',\n",
       "   ' mi',\n",
       "   'ett',\n",
       "   ' se',\n",
       "   ' le',\n",
       "   'do ',\n",
       "   'gli',\n",
       "   'e l',\n",
       "   ' ma',\n",
       "   'ion',\n",
       "   ' pa',\n",
       "   'iam',\n",
       "   'amo',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'ess',\n",
       "   'era',\n",
       "   'lla',\n",
       "   'i p',\n",
       "   'io ',\n",
       "   'o l',\n",
       "   'e n',\n",
       "   ' ca',\n",
       "   'son',\n",
       "   ' si',\n",
       "   'a a',\n",
       "   'ei ',\n",
       "   ' pi',\n",
       "   'a m',\n",
       "   'ai ',\n",
       "   'ver',\n",
       "   'men',\n",
       "   'ha ',\n",
       "   'ia ',\n",
       "   'nda',\n",
       "   'i d',\n",
       "   'so ',\n",
       "   'i i',\n",
       "   'ne ',\n",
       "   ' ri',\n",
       "   ' da',\n",
       "   'tru',\n",
       "   'ndo',\n",
       "   ' al',\n",
       "   'que',\n",
       "   ' ve',\n",
       "   'uir',\n",
       "   'ann',\n",
       "   'rui',\n",
       "   'ate',\n",
       "   ' po',\n",
       "   'una',\n",
       "   'chi',\n",
       "   'zio',\n",
       "   ' mo',\n",
       "   'lo ',\n",
       "   'sa ',\n",
       "   'ran',\n",
       "   'ues',\n",
       "   ' vo',\n",
       "   'pre',\n",
       "   'a f',\n",
       "   'a i',\n",
       "   ' vi',\n",
       "   'tat',\n",
       "   'n v',\n",
       "   'Io ',\n",
       "   ' To',\n",
       "   'li ',\n",
       "   'del',\n",
       "   'par',\n",
       "   'nno',\n",
       "   'one',\n",
       "   'si ',\n",
       "   'nti',\n",
       "   'att',\n",
       "   'qua',\n",
       "   'ano',\n",
       "   'a l',\n",
       "   'tra',\n",
       "   'se ',\n",
       "   'all',\n",
       "   ' do',\n",
       "   'ali',\n",
       "   ' ne',\n",
       "   'oi ',\n",
       "   'ant',\n",
       "   'o n',\n",
       "   ' va',\n",
       "   'mi ',\n",
       "   ' me',\n",
       "   'olt',\n",
       "   ' tu',\n",
       "   'va ',\n",
       "   'man',\n",
       "   'ero',\n",
       "   'ete',\n",
       "   ' lo',\n",
       "   'tro',\n",
       "   'sse',\n",
       "   'gio',\n",
       "   'com',\n",
       "   'osa',\n",
       "   'ora',\n",
       "   'n a',\n",
       "   'ata',\n",
       "   'azi',\n",
       "   'far',\n",
       "   'eri',\n",
       "   'n s',\n",
       "   'o m',\n",
       "   'da ',\n",
       "   'oro',\n",
       "   'o u',\n",
       "   'n p',\n",
       "   'res',\n",
       "   'sso',\n",
       "   'pia',\n",
       "   'anc',\n",
       "   'pro',\n",
       "   'erc',\n",
       "   ' er']}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dictionary of the features for all the languages \n",
    "lang = ['eng','deu','spa','fra','por','ita']\n",
    "n_list = ['50','100','200']\n",
    "lang_trigrams = {}\n",
    "for l in lang:\n",
    "    lang_trigrams[l] = lang_features(n_list,l)\n",
    "    print(l)\n",
    "lang_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "309a3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the lang_trigrams select list of unique trigrams i.e. final feature list\n",
    "features = {} #final feature list\n",
    "for n in n_list:\n",
    "    n_trigrams = []\n",
    "    for l in lang:\n",
    "            n_trigrams = n_trigrams + lang_trigrams[l][n]\n",
    "    features[n] = sorted(list(set(n_trigrams)))\n",
    "\n",
    "with open('data/features.json', 'w') as outfile:\n",
    "    json.dump(features, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65d70991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' To', ' a ', ' an', ' au', ' be', ' ca', ' ch', ' co', ' da', ' de', ' di', ' do', ' ei', ' el', ' en', ' es', ' ge', ' ha', ' he', ' in', ' is', ' la', ' le', ' lo', ' ma', ' me', ' mi', ' ne', ' ni', ' no', ' nã', ' o ', ' of', ' pa', ' pe', ' po', ' qu', ' se', ' si', ' so', ' st', ' te', ' th', ' to', ' um', ' un', ' vo', ' wa', ' wi', ' yo', ' zu', ' à ', ' è ', ' é ', 'Eu ', 'Ich', 'Je ', 'Mar', 'Non', 'Tom', 'a c', 'a d', 'a e', 'a s', 'ach', 'ado', 'ais', 'ait', 'and', 'ar ', 'ara', 'are', 'ary', 'as ', 'at ', 'ato', 'ch ', 'che', 'cht', 'com', 'con', 'cos', 'cê ', 'd t', 'da ', 'das', 'de ', 'den', 'der', 'di ', 'die', 'do ', 'e a', 'e c', 'e d', 'e e', 'e i', 'e l', 'e n', 'e p', 'e s', 'e t', 'e w', 'ed ', 'ein', 'el ', 'em ', 'en ', 'ent', 'er ', 'ere', 'es ', 'est', 'eu ', 'gen', 'hat', 'he ', 'hen', 'her', 'ht ', 'i s', 'ia ', 'ich', 'ie ', 'ien', 'in ', 'ine', 'ing', 'is ', 'ist', 'it ', 'la ', 'le ', 'les', 'll ', 'lle', 'n d', 'na ', 'nd ', 'ne ', 'nen', 'ng ', 'nic', 'no ', 'non', 'ns ', 'nt ', 'nte', 'não', 'o a', 'o c', 'o d', 'o e', 'o s', 'ocê', 'om ', 'on ', 'ono', 'or ', 'os ', 'ost', 'ou ', 'our', 'ous', 'par', 'pas', 'per', 'que', 'ra ', 're ', 'ry ', 's a', 's d', 's p', 'sch', 'se ', 'st ', 'sta', 'str', 't d', 't t', 'ta ', 'te ', 'ten', 'ter', 'tha', 'the', 'thi', 'to ', 'ue ', 'un ', 'und', 'us ', 've ', 'was', 'you', 'ão ']\n"
     ]
    }
   ],
   "source": [
    "with open('data/features.json', encoding='utf-8') as data_file:\n",
    "    features = json.loads(data_file.read())\n",
    "print(features['50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d072db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(sent,feature_list):\n",
    "    \"Takes in a list trigram (sentence in trigram form) and returns a feature in vector form, given a list of features\"\n",
    "    vector = [sent.count(f) for f in feature_list]\n",
    "    return(vector)\n",
    "print(vectorize([' Ma', ' To', ' a ', ' an', ' au', \" Ma\", \"e t\",'ccc','ach','st ','ach'],features['50']))\n",
    "print(vectorize([' Ma', ' To', ' a ', ' an', ' au', \" Ma\", \"e t\",'ccc','ach','st ','ach'],features['100']))\n",
    "print(vectorize([\"aaa\", \"bbb\",'eee','aaa','ccc','aaa'],features['50']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93fe22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(name,number):\n",
    "    \"Creates the apporiate feature matrix\"\n",
    "    mat = pd.read_csv(\"data/{}.csv\".format(name))\n",
    "    mat.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "    \n",
    "    sent_list = mat['sent']\n",
    "    sent_clean = list(map(clean_text, sent_list))\n",
    "    sent_trigram = list(map(char_trigram,sent_clean))\n",
    "    lang = mat['lang']\n",
    "    \n",
    "    vectors = [vectorize(s,features[number]) for s in sent_trigram]\n",
    "    df = pd.DataFrame(vectors, columns=features[number])\n",
    "    \n",
    "    df['lang'] = lang\n",
    "    df.to_csv('data/features/{}_{}.csv'.format(name,number))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a66769a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>To</th>\n",
       "      <th>a</th>\n",
       "      <th>an</th>\n",
       "      <th>au</th>\n",
       "      <th>be</th>\n",
       "      <th>ca</th>\n",
       "      <th>ch</th>\n",
       "      <th>co</th>\n",
       "      <th>da</th>\n",
       "      <th>de</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>ue</th>\n",
       "      <th>un</th>\n",
       "      <th>und</th>\n",
       "      <th>us</th>\n",
       "      <th>ve</th>\n",
       "      <th>was</th>\n",
       "      <th>you</th>\n",
       "      <th>ão</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        To   a    an   au   be   ca   ch   co   da   de  ...  to   ue   un   \\\n",
       "0        0    0    0    0    0    1    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    1  ...    1    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    1  ...    0    0    0   \n",
       "3        0    0    0    0    0    1    0    0    0    0  ...    0    0    0   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "29995    0    0    0    0    0    0    0    0    0    2  ...    0    0    0   \n",
       "29996    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "29997    0    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
       "29998    0    1    0    0    1    0    0    0    0    0  ...    0    0    0   \n",
       "29999    0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "\n",
       "       und  us   ve   was  you  ão   lang  \n",
       "0        0    0    1    0    1    0   eng  \n",
       "1        0    0    0    0    0    0   spa  \n",
       "2        0    0    0    0    0    0   eng  \n",
       "3        0    0    0    0    1    0   eng  \n",
       "4        0    0    1    0    0    0   ita  \n",
       "...    ...  ...  ...  ...  ...  ...   ...  \n",
       "29995    0    0    0    0    0    0   spa  \n",
       "29996    0    0    0    0    1    0   eng  \n",
       "29997    0    0    0    0    0    0   eng  \n",
       "29998    0    0    0    0    0    0   fra  \n",
       "29999    0    0    0    0    0    0   spa  \n",
       "\n",
       "[30000 rows x 198 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create feature vector matrix size 50\n",
    "create_features('train','50')\n",
    "create_features('valid','50')\n",
    "create_features('test','50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77cdd9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ge</th>\n",
       "      <th>I</th>\n",
       "      <th>Ma</th>\n",
       "      <th>Sc</th>\n",
       "      <th>Si</th>\n",
       "      <th>To</th>\n",
       "      <th>a</th>\n",
       "      <th>ac</th>\n",
       "      <th>al</th>\n",
       "      <th>an</th>\n",
       "      <th>...</th>\n",
       "      <th>y w</th>\n",
       "      <th>you</th>\n",
       "      <th>zio</th>\n",
       "      <th>zu</th>\n",
       "      <th>ás</th>\n",
       "      <th>ão</th>\n",
       "      <th>ère</th>\n",
       "      <th>ía</th>\n",
       "      <th>ón</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 678 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ge   I    Ma   Sc   Si   To   a    ac   al   an  ...  y w  you  zio  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "29995    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "29996    0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "29997    0    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
       "29998    0    0    0    0    0    0    1    0    0    0  ...    0    0    0   \n",
       "29999    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       zu   ás   ão   ère  ía   ón   lang  \n",
       "0        0    0    0    0    0    0   eng  \n",
       "1        0    0    0    0    0    0   spa  \n",
       "2        0    0    0    0    0    0   eng  \n",
       "3        0    0    0    0    0    0   eng  \n",
       "4        0    0    0    0    0    0   ita  \n",
       "...    ...  ...  ...  ...  ...  ...   ...  \n",
       "29995    0    1    0    0    0    0   spa  \n",
       "29996    0    0    0    0    0    0   eng  \n",
       "29997    0    0    0    0    0    0   eng  \n",
       "29998    0    0    0    0    0    0   fra  \n",
       "29999    0    0    0    0    0    0   spa  \n",
       "\n",
       "[30000 rows x 678 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size 100\n",
    "create_features('train','100')\n",
    "create_features('valid','100')\n",
    "create_features('test','100')\n",
    "\n",
    "#size 200\n",
    "create_features('train','200')\n",
    "create_features('valid','200')\n",
    "create_features('test','200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb963d6",
   "metadata": {},
   "source": [
    "## Create Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "296fda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d10bca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>I guess we have no choice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>« Où est Tom ? » – « Il est à la maison. »</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deu</td>\n",
       "      <td>Er spricht unsere Sprache nicht.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>por</td>\n",
       "      <td>Eu conheço Tom melhor do que você conhece.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Ein edler Mensch widmet sich dem Erreichen hoh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                               sent\n",
       "0  eng                         I guess we have no choice.\n",
       "1  fra         « Où est Tom ? » – « Il est à la maison. »\n",
       "2  deu                   Er spricht unsere Sprache nicht.\n",
       "3  por         Eu conheço Tom melhor do que você conhece.\n",
       "4  deu  Ein edler Mensch widmet sich dem Erreichen hoh..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "925523df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello This function Removes numbers punctuation and normalizes spaces'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove punctuation and numbers\n",
    "def clean_text(sent):\n",
    "    \"Takes in a string and returns it with no numbers or punctuation and normalized spaces\"\n",
    "    remove=string.punctuation + \"1234567890\" #Characters to be removed\n",
    "    table=str.maketrans(\"\",\"\",remove)    \n",
    "    sent = sent.translate(table)  \n",
    "    sent = \" \".join(sent.split()) #Normalize spaces\n",
    "    return sent\n",
    "\n",
    "sent = \"Hello. #This function908 Removes  numbers12,    punctuation... and     normalizes spaces\"\n",
    "clean_text(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "af179f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thi',\n",
       " 'his',\n",
       " 'is ',\n",
       " 's i',\n",
       " ' is',\n",
       " 'is ',\n",
       " 's a',\n",
       " ' a ',\n",
       " 'a s',\n",
       " ' se',\n",
       " 'sen',\n",
       " 'ent',\n",
       " 'nte',\n",
       " 'ten',\n",
       " 'enc',\n",
       " 'nce',\n",
       " 'ce.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_trigram(sent):\n",
    "    \"Takes a string and returns a list of character n-grams\"\n",
    "    return [sent[i:i+3] for i in range(len(sent)-3+1)]\n",
    "\n",
    "sent = \"This is a sentence.\"\n",
    "char_trigram(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "56700556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I g',\n",
       " ' gu',\n",
       " 'gue',\n",
       " 'ues',\n",
       " 'ess',\n",
       " 'ss ',\n",
       " 's w',\n",
       " ' we',\n",
       " 'we ',\n",
       " 'e h',\n",
       " ' ha',\n",
       " 'hav',\n",
       " 'ave',\n",
       " 've ',\n",
       " 'e n',\n",
       " ' no',\n",
       " 'no ',\n",
       " 'o c',\n",
       " ' ch',\n",
       " 'cho',\n",
       " 'hoi',\n",
       " 'oic',\n",
       " 'ice',\n",
       " '« O',\n",
       " ' Où',\n",
       " 'Où ',\n",
       " 'ù e',\n",
       " ' es',\n",
       " 'est',\n",
       " 'st ',\n",
       " 't T',\n",
       " ' To',\n",
       " 'Tom',\n",
       " 'om ',\n",
       " 'm »',\n",
       " ' » ',\n",
       " '» –',\n",
       " ' – ',\n",
       " '– «',\n",
       " ' « ',\n",
       " '« I',\n",
       " ' Il',\n",
       " 'Il ',\n",
       " 'l e',\n",
       " ' es',\n",
       " 'est',\n",
       " 'st ',\n",
       " 't à',\n",
       " ' à ',\n",
       " 'à l',\n",
       " ' la',\n",
       " 'la ',\n",
       " 'a m',\n",
       " ' ma',\n",
       " 'mai',\n",
       " 'ais',\n",
       " 'iso',\n",
       " 'son',\n",
       " 'on ',\n",
       " 'n »',\n",
       " 'Er ',\n",
       " 'r s',\n",
       " ' sp',\n",
       " 'spr',\n",
       " 'pri',\n",
       " 'ric',\n",
       " 'ich',\n",
       " 'cht',\n",
       " 'ht ',\n",
       " 't u',\n",
       " ' un',\n",
       " 'uns',\n",
       " 'nse',\n",
       " 'ser',\n",
       " 'ere',\n",
       " 're ',\n",
       " 'e S',\n",
       " ' Sp',\n",
       " 'Spr',\n",
       " 'pra',\n",
       " 'rac',\n",
       " 'ach',\n",
       " 'che',\n",
       " 'he ',\n",
       " 'e n',\n",
       " ' ni',\n",
       " 'nic',\n",
       " 'ich',\n",
       " 'cht',\n",
       " 'Eu ',\n",
       " 'u c',\n",
       " ' co',\n",
       " 'con',\n",
       " 'onh',\n",
       " 'nhe',\n",
       " 'heç',\n",
       " 'eço',\n",
       " 'ço ',\n",
       " 'o T',\n",
       " ' To',\n",
       " 'Tom',\n",
       " 'om ',\n",
       " 'm m',\n",
       " ' me',\n",
       " 'mel',\n",
       " 'elh',\n",
       " 'lho',\n",
       " 'hor',\n",
       " 'or ',\n",
       " 'r d',\n",
       " ' do',\n",
       " 'do ',\n",
       " 'o q',\n",
       " ' qu',\n",
       " 'que',\n",
       " 'ue ',\n",
       " 'e v',\n",
       " ' vo',\n",
       " 'voc',\n",
       " 'ocê',\n",
       " 'cê ',\n",
       " 'ê c',\n",
       " ' co',\n",
       " 'con',\n",
       " 'onh',\n",
       " 'nhe',\n",
       " 'hec',\n",
       " 'ece',\n",
       " 'Ein',\n",
       " 'in ',\n",
       " 'n e',\n",
       " ' ed',\n",
       " 'edl',\n",
       " 'dle',\n",
       " 'ler',\n",
       " 'er ',\n",
       " 'r M',\n",
       " ' Me',\n",
       " 'Men',\n",
       " 'ens',\n",
       " 'nsc',\n",
       " 'sch',\n",
       " 'ch ',\n",
       " 'h w',\n",
       " ' wi',\n",
       " 'wid',\n",
       " 'idm',\n",
       " 'dme',\n",
       " 'met',\n",
       " 'et ',\n",
       " 't s',\n",
       " ' si',\n",
       " 'sic',\n",
       " 'ich',\n",
       " 'ch ',\n",
       " 'h d',\n",
       " ' de',\n",
       " 'dem',\n",
       " 'em ',\n",
       " 'm E',\n",
       " ' Er',\n",
       " 'Err',\n",
       " 'rre',\n",
       " 'rei',\n",
       " 'eic',\n",
       " 'ich',\n",
       " 'che',\n",
       " 'hen',\n",
       " 'en ',\n",
       " 'n h',\n",
       " ' ho',\n",
       " 'hoh',\n",
       " 'ohe',\n",
       " 'her',\n",
       " 'er ',\n",
       " 'r Z',\n",
       " ' Zi',\n",
       " 'Zie',\n",
       " 'iel',\n",
       " 'ele',\n",
       " 'Ela',\n",
       " 'la ',\n",
       " 'a f',\n",
       " ' fa',\n",
       " 'faz',\n",
       " 'az ',\n",
       " 'z p',\n",
       " ' pa',\n",
       " 'par',\n",
       " 'art',\n",
       " 'rte',\n",
       " 'te ',\n",
       " 'e d',\n",
       " ' do',\n",
       " 'do ',\n",
       " 'o c',\n",
       " ' cl',\n",
       " 'clu',\n",
       " 'lub',\n",
       " 'ube',\n",
       " 'be ',\n",
       " 'e d',\n",
       " ' de',\n",
       " 'de ',\n",
       " 'e t',\n",
       " ' tê',\n",
       " 'tên',\n",
       " 'êni',\n",
       " 'nis',\n",
       " 'Mei',\n",
       " 'ein',\n",
       " 'in ',\n",
       " 'n A',\n",
       " ' Au',\n",
       " 'Aut',\n",
       " 'uto',\n",
       " 'to ',\n",
       " 'o i',\n",
       " ' is',\n",
       " 'ist',\n",
       " 'st ',\n",
       " 't v',\n",
       " ' ve',\n",
       " 'ver',\n",
       " 'ers',\n",
       " 'rsi',\n",
       " 'sic',\n",
       " 'ich',\n",
       " 'che',\n",
       " 'her',\n",
       " 'ert',\n",
       " 'Mar',\n",
       " 'ary',\n",
       " 'ry ',\n",
       " 'y t',\n",
       " ' te',\n",
       " 'tem',\n",
       " 'em ',\n",
       " 'm u',\n",
       " ' um',\n",
       " 'um ',\n",
       " 'm n',\n",
       " ' na',\n",
       " 'nam',\n",
       " 'amo',\n",
       " 'mor',\n",
       " 'ora',\n",
       " 'rad',\n",
       " 'ado',\n",
       " 'do ',\n",
       " 'o s',\n",
       " ' se',\n",
       " 'sec',\n",
       " 'ecr',\n",
       " 'cre',\n",
       " 'ret',\n",
       " 'eto',\n",
       " 'Emi',\n",
       " 'mil',\n",
       " 'ily',\n",
       " 'ly ',\n",
       " 'y v',\n",
       " ' vi',\n",
       " 'vis',\n",
       " 'isi',\n",
       " 'sit',\n",
       " 'ita',\n",
       " 'tar',\n",
       " 'ará',\n",
       " 'rá ',\n",
       " 'á o',\n",
       " ' os',\n",
       " 'os ',\n",
       " 's p',\n",
       " ' pa',\n",
       " 'pai',\n",
       " 'ais',\n",
       " 'is ',\n",
       " 's d',\n",
       " ' de',\n",
       " 'del',\n",
       " 'ela',\n",
       " 'Auc',\n",
       " 'uch',\n",
       " 'ch ',\n",
       " 'h e',\n",
       " ' ei',\n",
       " 'ein',\n",
       " 'ine',\n",
       " 'ne ',\n",
       " 'e P',\n",
       " ' Ps',\n",
       " 'Psy',\n",
       " 'syc',\n",
       " 'ych',\n",
       " 'cho',\n",
       " 'hol',\n",
       " 'olo',\n",
       " 'log',\n",
       " 'ogi',\n",
       " 'gin',\n",
       " 'in ',\n",
       " 'n g',\n",
       " ' ge',\n",
       " 'ger',\n",
       " 'erä',\n",
       " 'rät',\n",
       " 'ät ',\n",
       " 't a',\n",
       " ' an',\n",
       " 'an ',\n",
       " 'n K',\n",
       " ' Ko',\n",
       " 'Kol',\n",
       " 'oll',\n",
       " 'lle',\n",
       " 'leg',\n",
       " 'ege',\n",
       " 'gen',\n",
       " 'en ',\n",
       " 'n d',\n",
       " ' di',\n",
       " 'die',\n",
       " 'ie ',\n",
       " 'e i',\n",
       " ' ih',\n",
       " 'ihr',\n",
       " 'hr ',\n",
       " 'r m',\n",
       " ' me',\n",
       " 'meh',\n",
       " 'ehr',\n",
       " 'hr ',\n",
       " 'r s',\n",
       " ' sc',\n",
       " 'sch',\n",
       " 'cha',\n",
       " 'had',\n",
       " 'ade',\n",
       " 'den',\n",
       " 'en ',\n",
       " 'n a',\n",
       " ' al',\n",
       " 'als',\n",
       " 'ls ',\n",
       " 's h',\n",
       " ' he',\n",
       " 'hel',\n",
       " 'elf',\n",
       " 'lfe',\n",
       " 'fen']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trigram_list(sent_list):\n",
    "    \"Takes in a list of sentences and returns a list of trigrams \"\n",
    "    sent_clean = list(map(clean_text, sent_list))\n",
    "    sent_trigram = list(map(char_trigram,sent_clean))\n",
    "    list_trigram = [item for sublist in sent_trigram for item in sublist]\n",
    "    return list_trigram\n",
    "\n",
    "sent_list = train['sent'][0:10]\n",
    "trigram_list(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8d4f5797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' de',\n",
       " 'er ',\n",
       " 'as ',\n",
       " 'en ',\n",
       " 'es ',\n",
       " ' qu',\n",
       " 'de ',\n",
       " 'que',\n",
       " 'te ',\n",
       " ' co',\n",
       " 're ',\n",
       " ' a ',\n",
       " 'est',\n",
       " 'Tom',\n",
       " 'om ',\n",
       " 'ue ',\n",
       " 'ch ',\n",
       " 'ent',\n",
       " ' pa',\n",
       " 'to ']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_frequent(n,trigrams):\n",
    "    \"Takes in a list of trigrams and returns the n most frequent trigrams\"\n",
    "    common = []\n",
    "    for e in Counter(trigrams).most_common(n):\n",
    "        common.append(e[0])\n",
    "    return common\n",
    "\n",
    "sent_list = train['sent'][0:1000]\n",
    "trigrams = trigram_list(sent_list)\n",
    "most_frequent(20, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b311a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20': [' th',\n",
       "  'he ',\n",
       "  ' to',\n",
       "  'the',\n",
       "  'to ',\n",
       "  'om ',\n",
       "  'Tom',\n",
       "  'hat',\n",
       "  'nt ',\n",
       "  'ing',\n",
       "  'ed ',\n",
       "  'at ',\n",
       "  'is ',\n",
       "  'ng ',\n",
       "  'tha',\n",
       "  ' do',\n",
       "  ' wa',\n",
       "  ' yo',\n",
       "  'you',\n",
       "  'e t'],\n",
       " '30': [' th',\n",
       "  'he ',\n",
       "  ' to',\n",
       "  'the',\n",
       "  'to ',\n",
       "  'om ',\n",
       "  'Tom',\n",
       "  'hat',\n",
       "  'nt ',\n",
       "  'ing',\n",
       "  'ed ',\n",
       "  'at ',\n",
       "  'is ',\n",
       "  'ng ',\n",
       "  'tha',\n",
       "  ' do',\n",
       "  ' wa',\n",
       "  ' yo',\n",
       "  'you',\n",
       "  'e t',\n",
       "  're ',\n",
       "  ' a ',\n",
       "  ' an',\n",
       "  ' ha',\n",
       "  ' he',\n",
       "  'as ',\n",
       "  't t',\n",
       "  'er ',\n",
       "  'd t',\n",
       "  'nd ']}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lang_features(n_list,lang):\n",
    "    \"\"\"Returns a dictionary of the most frequent trigrams for a given language. Each element is a list of the n most\n",
    "    frequent trigrams when n is a element of n_list\"\"\"\n",
    "    \n",
    "    train_lang = train[train['lang'] == lang]\n",
    "    sent_list = train_lang['sent']\n",
    "    trigrams = trigram_list(sent_list)\n",
    "    \n",
    "    freq = {}\n",
    "    for n in n_list:\n",
    "        freq[n] = most_frequent(int(n), trigrams)\n",
    "    return freq \n",
    "    \n",
    "lang_features(['20','30'],'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3d6cca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "deu\n",
      "spa\n",
      "fra\n",
      "por\n",
      "ita\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eng': {'50': [' th',\n",
       "   'he ',\n",
       "   ' to',\n",
       "   'the',\n",
       "   'to ',\n",
       "   'om ',\n",
       "   'Tom',\n",
       "   'hat',\n",
       "   'nt ',\n",
       "   'ing',\n",
       "   'ed ',\n",
       "   'at ',\n",
       "   'is ',\n",
       "   'ng ',\n",
       "   'tha',\n",
       "   ' do',\n",
       "   ' wa',\n",
       "   ' yo',\n",
       "   'you',\n",
       "   'e t',\n",
       "   're ',\n",
       "   ' a ',\n",
       "   ' an',\n",
       "   ' ha',\n",
       "   ' he',\n",
       "   'as ',\n",
       "   't t',\n",
       "   'er ',\n",
       "   'd t',\n",
       "   'nd ',\n",
       "   ' is',\n",
       "   ' in',\n",
       "   'ry ',\n",
       "   'her',\n",
       "   'ou ',\n",
       "   ' be',\n",
       "   'and',\n",
       "   've ',\n",
       "   'in ',\n",
       "   'thi',\n",
       "   ' of',\n",
       "   'll ',\n",
       "   'ary',\n",
       "   'e a',\n",
       "   'ere',\n",
       "   'Mar',\n",
       "   's a',\n",
       "   'was',\n",
       "   'e w',\n",
       "   ' To'],\n",
       "  '100': [' th',\n",
       "   'he ',\n",
       "   ' to',\n",
       "   'the',\n",
       "   'to ',\n",
       "   'om ',\n",
       "   'Tom',\n",
       "   'hat',\n",
       "   'nt ',\n",
       "   'ing',\n",
       "   'ed ',\n",
       "   'at ',\n",
       "   'is ',\n",
       "   'ng ',\n",
       "   'tha',\n",
       "   ' do',\n",
       "   ' wa',\n",
       "   ' yo',\n",
       "   'you',\n",
       "   'e t',\n",
       "   're ',\n",
       "   ' a ',\n",
       "   ' an',\n",
       "   ' ha',\n",
       "   ' he',\n",
       "   'as ',\n",
       "   't t',\n",
       "   'er ',\n",
       "   'd t',\n",
       "   'nd ',\n",
       "   ' is',\n",
       "   ' in',\n",
       "   'ry ',\n",
       "   'her',\n",
       "   'ou ',\n",
       "   ' be',\n",
       "   'and',\n",
       "   've ',\n",
       "   'in ',\n",
       "   'thi',\n",
       "   ' of',\n",
       "   'll ',\n",
       "   'ary',\n",
       "   'e a',\n",
       "   'ere',\n",
       "   'Mar',\n",
       "   's a',\n",
       "   'was',\n",
       "   'e w',\n",
       "   ' To',\n",
       "   'o t',\n",
       "   'me ',\n",
       "   'his',\n",
       "   's t',\n",
       "   'es ',\n",
       "   'of ',\n",
       "   ' co',\n",
       "   ' wi',\n",
       "   'e s',\n",
       "   ' hi',\n",
       "   'n t',\n",
       "   'ver',\n",
       "   'hin',\n",
       "   'The',\n",
       "   ' di',\n",
       "   'ld ',\n",
       "   ' wh',\n",
       "   ' Ma',\n",
       "   ' ca',\n",
       "   'on ',\n",
       "   ' go',\n",
       "   'ow ',\n",
       "   ' sa',\n",
       "   ' fo',\n",
       "   'en ',\n",
       "   ' we',\n",
       "   'for',\n",
       "   ' on',\n",
       "   'an ',\n",
       "   'ly ',\n",
       "   ' me',\n",
       "   't h',\n",
       "   'do ',\n",
       "   'ave',\n",
       "   ' wo',\n",
       "   'ent',\n",
       "   'ant',\n",
       "   ' sh',\n",
       "   ' re',\n",
       "   'id ',\n",
       "   ' st',\n",
       "   'oul',\n",
       "   ' li',\n",
       "   'all',\n",
       "   'e h',\n",
       "   ' no',\n",
       "   'or ',\n",
       "   'st ',\n",
       "   'e i',\n",
       "   'uld'],\n",
       "  '200': [' th',\n",
       "   'he ',\n",
       "   ' to',\n",
       "   'the',\n",
       "   'to ',\n",
       "   'om ',\n",
       "   'Tom',\n",
       "   'hat',\n",
       "   'nt ',\n",
       "   'ing',\n",
       "   'ed ',\n",
       "   'at ',\n",
       "   'is ',\n",
       "   'ng ',\n",
       "   'tha',\n",
       "   ' do',\n",
       "   ' wa',\n",
       "   ' yo',\n",
       "   'you',\n",
       "   'e t',\n",
       "   're ',\n",
       "   ' a ',\n",
       "   ' an',\n",
       "   ' ha',\n",
       "   ' he',\n",
       "   'as ',\n",
       "   't t',\n",
       "   'er ',\n",
       "   'd t',\n",
       "   'nd ',\n",
       "   ' is',\n",
       "   ' in',\n",
       "   'ry ',\n",
       "   'her',\n",
       "   'ou ',\n",
       "   ' be',\n",
       "   'and',\n",
       "   've ',\n",
       "   'in ',\n",
       "   'thi',\n",
       "   ' of',\n",
       "   'll ',\n",
       "   'ary',\n",
       "   'e a',\n",
       "   'ere',\n",
       "   'Mar',\n",
       "   's a',\n",
       "   'was',\n",
       "   'e w',\n",
       "   ' To',\n",
       "   'o t',\n",
       "   'me ',\n",
       "   'his',\n",
       "   's t',\n",
       "   'es ',\n",
       "   'of ',\n",
       "   ' co',\n",
       "   ' wi',\n",
       "   'e s',\n",
       "   ' hi',\n",
       "   'n t',\n",
       "   'ver',\n",
       "   'hin',\n",
       "   'The',\n",
       "   ' di',\n",
       "   'ld ',\n",
       "   ' wh',\n",
       "   ' Ma',\n",
       "   ' ca',\n",
       "   'on ',\n",
       "   ' go',\n",
       "   'ow ',\n",
       "   ' sa',\n",
       "   ' fo',\n",
       "   'en ',\n",
       "   ' we',\n",
       "   'for',\n",
       "   ' on',\n",
       "   'an ',\n",
       "   'ly ',\n",
       "   ' me',\n",
       "   't h',\n",
       "   'do ',\n",
       "   'ave',\n",
       "   ' wo',\n",
       "   'ent',\n",
       "   'ant',\n",
       "   ' sh',\n",
       "   ' re',\n",
       "   'id ',\n",
       "   ' st',\n",
       "   'oul',\n",
       "   ' li',\n",
       "   'all',\n",
       "   'e h',\n",
       "   ' no',\n",
       "   'or ',\n",
       "   'st ',\n",
       "   'e i',\n",
       "   'uld',\n",
       "   'ut ',\n",
       "   'are',\n",
       "   'ts ',\n",
       "   'one',\n",
       "   'g t',\n",
       "   't w',\n",
       "   't a',\n",
       "   ' I ',\n",
       "   'ont',\n",
       "   'now',\n",
       "   'ot ',\n",
       "   'ter',\n",
       "   'our',\n",
       "   ' kn',\n",
       "   'ght',\n",
       "   ' ne',\n",
       "   'eve',\n",
       "   'e d',\n",
       "   'th ',\n",
       "   ' so',\n",
       "   ' ar',\n",
       "   'e c',\n",
       "   'ome',\n",
       "   'hav',\n",
       "   'did',\n",
       "   'hou',\n",
       "   'y t',\n",
       "   'be ',\n",
       "   'o d',\n",
       "   'e o',\n",
       "   'e b',\n",
       "   'dnt',\n",
       "   'rea',\n",
       "   ' it',\n",
       "   'm a',\n",
       "   ' ma',\n",
       "   'ith',\n",
       "   'ted',\n",
       "   ' ho',\n",
       "   'ke ',\n",
       "   ' lo',\n",
       "   'hey',\n",
       "   'out',\n",
       "   'wit',\n",
       "   'd a',\n",
       "   't s',\n",
       "   'kno',\n",
       "   'ey ',\n",
       "   'le ',\n",
       "   'ill',\n",
       "   'snt',\n",
       "   'ne ',\n",
       "   's s',\n",
       "   ' mo',\n",
       "   'e m',\n",
       "   ' al',\n",
       "   ' se',\n",
       "   'ion',\n",
       "   's i',\n",
       "   'd h',\n",
       "   'y w',\n",
       "   'not',\n",
       "   'don',\n",
       "   'it ',\n",
       "   'wan',\n",
       "   'ate',\n",
       "   'se ',\n",
       "   'r t',\n",
       "   'aid',\n",
       "   'ay ',\n",
       "   'y s',\n",
       "   'ht ',\n",
       "   'd M',\n",
       "   'ink',\n",
       "   'm w',\n",
       "   'ad ',\n",
       "   'ear',\n",
       "   ' pr',\n",
       "   't o',\n",
       "   'e f',\n",
       "   'et ',\n",
       "   'ery',\n",
       "   't i',\n",
       "   'she',\n",
       "   'ive',\n",
       "   'He ',\n",
       "   'e p',\n",
       "   's w',\n",
       "   'n a',\n",
       "   ' at',\n",
       "   'sai',\n",
       "   'oin',\n",
       "   'm i',\n",
       "   'ugh',\n",
       "   'ur ',\n",
       "   ' as',\n",
       "   'y a',\n",
       "   'nk ',\n",
       "   'any',\n",
       "   's h']},\n",
       " 'deu': {'50': ['en ',\n",
       "   'ch ',\n",
       "   'ich',\n",
       "   'er ',\n",
       "   'ein',\n",
       "   'ie ',\n",
       "   'cht',\n",
       "   'st ',\n",
       "   'sch',\n",
       "   'in ',\n",
       "   ' de',\n",
       "   'ine',\n",
       "   ' ei',\n",
       "   'ht ',\n",
       "   'te ',\n",
       "   'che',\n",
       "   ' da',\n",
       "   ' di',\n",
       "   'as ',\n",
       "   'ist',\n",
       "   ' ge',\n",
       "   'der',\n",
       "   ' ni',\n",
       "   't d',\n",
       "   ' ha',\n",
       "   'nd ',\n",
       "   'nic',\n",
       "   ' is',\n",
       "   'Ich',\n",
       "   'die',\n",
       "   'den',\n",
       "   'es ',\n",
       "   ' zu',\n",
       "   'gen',\n",
       "   'Tom',\n",
       "   'hen',\n",
       "   ' si',\n",
       "   'n d',\n",
       "   'ne ',\n",
       "   'om ',\n",
       "   'das',\n",
       "   ' un',\n",
       "   ' mi',\n",
       "   'ten',\n",
       "   ' wi',\n",
       "   'und',\n",
       "   'nen',\n",
       "   'ach',\n",
       "   'ter',\n",
       "   ' au'],\n",
       "  '100': ['en ',\n",
       "   'ch ',\n",
       "   'ich',\n",
       "   'er ',\n",
       "   'ein',\n",
       "   'ie ',\n",
       "   'cht',\n",
       "   'st ',\n",
       "   'sch',\n",
       "   'in ',\n",
       "   ' de',\n",
       "   'ine',\n",
       "   ' ei',\n",
       "   'ht ',\n",
       "   'te ',\n",
       "   'che',\n",
       "   ' da',\n",
       "   ' di',\n",
       "   'as ',\n",
       "   'ist',\n",
       "   ' ge',\n",
       "   'der',\n",
       "   ' ni',\n",
       "   't d',\n",
       "   ' ha',\n",
       "   'nd ',\n",
       "   'nic',\n",
       "   ' is',\n",
       "   'Ich',\n",
       "   'die',\n",
       "   'den',\n",
       "   'es ',\n",
       "   ' zu',\n",
       "   'gen',\n",
       "   'Tom',\n",
       "   'hen',\n",
       "   ' si',\n",
       "   'n d',\n",
       "   'ne ',\n",
       "   'om ',\n",
       "   'das',\n",
       "   ' un',\n",
       "   ' mi',\n",
       "   'ten',\n",
       "   ' wi',\n",
       "   'und',\n",
       "   'nen',\n",
       "   'ach',\n",
       "   'ter',\n",
       "   ' au',\n",
       "   'nde',\n",
       "   'ir ',\n",
       "   'zu ',\n",
       "   ' se',\n",
       "   ' be',\n",
       "   'ste',\n",
       "   'ben',\n",
       "   'abe',\n",
       "   'ass',\n",
       "   'it ',\n",
       "   'e d',\n",
       "   ' we',\n",
       "   ' du',\n",
       "   'eit',\n",
       "   't e',\n",
       "   ' wa',\n",
       "   ' in',\n",
       "   'Sie',\n",
       "   ' er',\n",
       "   'ver',\n",
       "   ' ic',\n",
       "   'n s',\n",
       "   'nn ',\n",
       "   'sse',\n",
       "   'em ',\n",
       "   ' sc',\n",
       "   'e i',\n",
       "   'ss ',\n",
       "   'nge',\n",
       "   'auf',\n",
       "   'du ',\n",
       "   'sen',\n",
       "   'n w',\n",
       "   'ren',\n",
       "   'hat',\n",
       "   't s',\n",
       "   'e s',\n",
       "   ' ve',\n",
       "   'r d',\n",
       "   'ber',\n",
       "   'lle',\n",
       "   'hab',\n",
       "   'hr ',\n",
       "   'lic',\n",
       "   'ers',\n",
       "   'ann',\n",
       "   'ung',\n",
       "   'ere',\n",
       "   'ier',\n",
       "   's i'],\n",
       "  '200': ['en ',\n",
       "   'ch ',\n",
       "   'ich',\n",
       "   'er ',\n",
       "   'ein',\n",
       "   'ie ',\n",
       "   'cht',\n",
       "   'st ',\n",
       "   'sch',\n",
       "   'in ',\n",
       "   ' de',\n",
       "   'ine',\n",
       "   ' ei',\n",
       "   'ht ',\n",
       "   'te ',\n",
       "   'che',\n",
       "   ' da',\n",
       "   ' di',\n",
       "   'as ',\n",
       "   'ist',\n",
       "   ' ge',\n",
       "   'der',\n",
       "   ' ni',\n",
       "   't d',\n",
       "   ' ha',\n",
       "   'nd ',\n",
       "   'nic',\n",
       "   ' is',\n",
       "   'Ich',\n",
       "   'die',\n",
       "   'den',\n",
       "   'es ',\n",
       "   ' zu',\n",
       "   'gen',\n",
       "   'Tom',\n",
       "   'hen',\n",
       "   ' si',\n",
       "   'n d',\n",
       "   'ne ',\n",
       "   'om ',\n",
       "   'das',\n",
       "   ' un',\n",
       "   ' mi',\n",
       "   'ten',\n",
       "   ' wi',\n",
       "   'und',\n",
       "   'nen',\n",
       "   'ach',\n",
       "   'ter',\n",
       "   ' au',\n",
       "   'nde',\n",
       "   'ir ',\n",
       "   'zu ',\n",
       "   ' se',\n",
       "   ' be',\n",
       "   'ste',\n",
       "   'ben',\n",
       "   'abe',\n",
       "   'ass',\n",
       "   'it ',\n",
       "   'e d',\n",
       "   ' we',\n",
       "   ' du',\n",
       "   'eit',\n",
       "   't e',\n",
       "   ' wa',\n",
       "   ' in',\n",
       "   'Sie',\n",
       "   ' er',\n",
       "   'ver',\n",
       "   ' ic',\n",
       "   'n s',\n",
       "   'nn ',\n",
       "   'sse',\n",
       "   'em ',\n",
       "   ' sc',\n",
       "   'e i',\n",
       "   'ss ',\n",
       "   'nge',\n",
       "   'auf',\n",
       "   'du ',\n",
       "   'sen',\n",
       "   'n w',\n",
       "   'ren',\n",
       "   'hat',\n",
       "   't s',\n",
       "   'e s',\n",
       "   ' ve',\n",
       "   'r d',\n",
       "   'ber',\n",
       "   'lle',\n",
       "   'hab',\n",
       "   'hr ',\n",
       "   'lic',\n",
       "   'ers',\n",
       "   'ann',\n",
       "   'ung',\n",
       "   'ere',\n",
       "   'ier',\n",
       "   's i',\n",
       "   'mme',\n",
       "   'on ',\n",
       "   ' al',\n",
       "   'sei',\n",
       "   'uch',\n",
       "   'ern',\n",
       "   ' vo',\n",
       "   'aus',\n",
       "   'tte',\n",
       "   ' es',\n",
       "   ' ih',\n",
       "   'n S',\n",
       "   'ind',\n",
       "   'and',\n",
       "   'n i',\n",
       "   'hre',\n",
       "   ' an',\n",
       "   'ese',\n",
       "   'an ',\n",
       "   'r s',\n",
       "   'rde',\n",
       "   'at ',\n",
       "   'sic',\n",
       "   ' so',\n",
       "   'e e',\n",
       "   'de ',\n",
       "   'sie',\n",
       "   't i',\n",
       "   'n e',\n",
       "   'war',\n",
       "   ' ma',\n",
       "   'wir',\n",
       "   ' me',\n",
       "   'he ',\n",
       "   'Die',\n",
       "   'ges',\n",
       "   'mit',\n",
       "   'nte',\n",
       "   'um ',\n",
       "   't m',\n",
       "   'be ',\n",
       "   'och',\n",
       "   'est',\n",
       "   'lte',\n",
       "   'r e',\n",
       "   ' To',\n",
       "   'rei',\n",
       "   'hte',\n",
       "   'ehe',\n",
       "   'Sch',\n",
       "   's d',\n",
       "   'men',\n",
       "   'ies',\n",
       "   'enn',\n",
       "   ' Ma',\n",
       "   'h d',\n",
       "   'n a',\n",
       "   'e m',\n",
       "   'ert',\n",
       "   'ehr',\n",
       "   't w',\n",
       "   'wei',\n",
       "   'uf ',\n",
       "   't a',\n",
       "   ' Ge',\n",
       "   'h h',\n",
       "   'lei',\n",
       "   'ng ',\n",
       "   ' Sc',\n",
       "   ' ka',\n",
       "   'iel',\n",
       "   'h w',\n",
       "   ' bi',\n",
       "   're ',\n",
       "   'ebe',\n",
       "   'ige',\n",
       "   'ge ',\n",
       "   'Mar',\n",
       "   'ner',\n",
       "   'isc',\n",
       "   'all',\n",
       "   'ar ',\n",
       "   ' Si',\n",
       "   'n m',\n",
       "   'e S',\n",
       "   'end',\n",
       "   'r i',\n",
       "   't n',\n",
       "   'len',\n",
       "   'ari',\n",
       "   ' im',\n",
       "   'Er ',\n",
       "   'ger',\n",
       "   'was',\n",
       "   'hei',\n",
       "   'e n',\n",
       "   'ang',\n",
       "   'mei',\n",
       "   'n g',\n",
       "   'Das']},\n",
       " 'spa': {'50': [' de',\n",
       "   ' es',\n",
       "   'de ',\n",
       "   'os ',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'es ',\n",
       "   'ue ',\n",
       "   ' la',\n",
       "   'la ',\n",
       "   ' co',\n",
       "   'as ',\n",
       "   'do ',\n",
       "   'est',\n",
       "   ' en',\n",
       "   'en ',\n",
       "   ' un',\n",
       "   'el ',\n",
       "   ' a ',\n",
       "   'ent',\n",
       "   ' el',\n",
       "   'te ',\n",
       "   'no ',\n",
       "   ' no',\n",
       "   'o e',\n",
       "   'nte',\n",
       "   ' ha',\n",
       "   ' se',\n",
       "   'ra ',\n",
       "   'e e',\n",
       "   'a e',\n",
       "   'sta',\n",
       "   'ien',\n",
       "   'con',\n",
       "   ' pa',\n",
       "   'e l',\n",
       "   'na ',\n",
       "   'ar ',\n",
       "   'a c',\n",
       "   ' lo',\n",
       "   's d',\n",
       "   'ado',\n",
       "   ' ca',\n",
       "   'or ',\n",
       "   'o d',\n",
       "   'a d',\n",
       "   'se ',\n",
       "   ' po',\n",
       "   'un ',\n",
       "   'on '],\n",
       "  '100': [' de',\n",
       "   ' es',\n",
       "   'de ',\n",
       "   'os ',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'es ',\n",
       "   'ue ',\n",
       "   ' la',\n",
       "   'la ',\n",
       "   ' co',\n",
       "   'as ',\n",
       "   'do ',\n",
       "   'est',\n",
       "   ' en',\n",
       "   'en ',\n",
       "   ' un',\n",
       "   'el ',\n",
       "   ' a ',\n",
       "   'ent',\n",
       "   ' el',\n",
       "   'te ',\n",
       "   'no ',\n",
       "   ' no',\n",
       "   'o e',\n",
       "   'nte',\n",
       "   ' ha',\n",
       "   ' se',\n",
       "   'ra ',\n",
       "   'e e',\n",
       "   'a e',\n",
       "   'sta',\n",
       "   'ien',\n",
       "   'con',\n",
       "   ' pa',\n",
       "   'e l',\n",
       "   'na ',\n",
       "   'ar ',\n",
       "   'a c',\n",
       "   ' lo',\n",
       "   's d',\n",
       "   'ado',\n",
       "   ' ca',\n",
       "   'or ',\n",
       "   'o d',\n",
       "   'a d',\n",
       "   'se ',\n",
       "   ' po',\n",
       "   'un ',\n",
       "   'on ',\n",
       "   'Tom',\n",
       "   'ta ',\n",
       "   ' me',\n",
       "   'lo ',\n",
       "   'a l',\n",
       "   'a p',\n",
       "   's e',\n",
       "   'ndo',\n",
       "   'ía ',\n",
       "   'e d',\n",
       "   'ro ',\n",
       "   'n e',\n",
       "   'er ',\n",
       "   'o q',\n",
       "   'e a',\n",
       "   ' pe',\n",
       "   'a m',\n",
       "   ' al',\n",
       "   'par',\n",
       "   'to ',\n",
       "   'una',\n",
       "   'o p',\n",
       "   ' te',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'res',\n",
       "   'tra',\n",
       "   'los',\n",
       "   'om ',\n",
       "   'o s',\n",
       "   'ero',\n",
       "   'o a',\n",
       "   'e c',\n",
       "   's p',\n",
       "   ' pr',\n",
       "   ' di',\n",
       "   ' mu',\n",
       "   's a',\n",
       "   'por',\n",
       "   'a a',\n",
       "   'e s',\n",
       "   'ara',\n",
       "   'No ',\n",
       "   'a s',\n",
       "   'per',\n",
       "   'era',\n",
       "   'ier',\n",
       "   'me ',\n",
       "   'an ',\n",
       "   ' mi'],\n",
       "  '200': [' de',\n",
       "   ' es',\n",
       "   'de ',\n",
       "   'os ',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'es ',\n",
       "   'ue ',\n",
       "   ' la',\n",
       "   'la ',\n",
       "   ' co',\n",
       "   'as ',\n",
       "   'do ',\n",
       "   'est',\n",
       "   ' en',\n",
       "   'en ',\n",
       "   ' un',\n",
       "   'el ',\n",
       "   ' a ',\n",
       "   'ent',\n",
       "   ' el',\n",
       "   'te ',\n",
       "   'no ',\n",
       "   ' no',\n",
       "   'o e',\n",
       "   'nte',\n",
       "   ' ha',\n",
       "   ' se',\n",
       "   'ra ',\n",
       "   'e e',\n",
       "   'a e',\n",
       "   'sta',\n",
       "   'ien',\n",
       "   'con',\n",
       "   ' pa',\n",
       "   'e l',\n",
       "   'na ',\n",
       "   'ar ',\n",
       "   'a c',\n",
       "   ' lo',\n",
       "   's d',\n",
       "   'ado',\n",
       "   ' ca',\n",
       "   'or ',\n",
       "   'o d',\n",
       "   'a d',\n",
       "   'se ',\n",
       "   ' po',\n",
       "   'un ',\n",
       "   'on ',\n",
       "   'Tom',\n",
       "   'ta ',\n",
       "   ' me',\n",
       "   'lo ',\n",
       "   'a l',\n",
       "   'a p',\n",
       "   's e',\n",
       "   'ndo',\n",
       "   'ía ',\n",
       "   'e d',\n",
       "   'ro ',\n",
       "   'n e',\n",
       "   'er ',\n",
       "   'o q',\n",
       "   'e a',\n",
       "   ' pe',\n",
       "   'a m',\n",
       "   ' al',\n",
       "   'par',\n",
       "   'to ',\n",
       "   'una',\n",
       "   'o p',\n",
       "   ' te',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'res',\n",
       "   'tra',\n",
       "   'los',\n",
       "   'om ',\n",
       "   'o s',\n",
       "   'ero',\n",
       "   'o a',\n",
       "   'e c',\n",
       "   's p',\n",
       "   ' pr',\n",
       "   ' di',\n",
       "   ' mu',\n",
       "   's a',\n",
       "   'por',\n",
       "   'a a',\n",
       "   'e s',\n",
       "   'ara',\n",
       "   'No ',\n",
       "   'a s',\n",
       "   'per',\n",
       "   'era',\n",
       "   'ier',\n",
       "   'me ',\n",
       "   'an ',\n",
       "   ' mi',\n",
       "   'com',\n",
       "   'men',\n",
       "   ' ve',\n",
       "   'ant',\n",
       "   ' re',\n",
       "   ' y ',\n",
       "   ' ma',\n",
       "   'da ',\n",
       "   ' so',\n",
       "   's c',\n",
       "   'n l',\n",
       "   'mos',\n",
       "   'lla',\n",
       "   'aba',\n",
       "   ' si',\n",
       "   'las',\n",
       "   'stá',\n",
       "   'e t',\n",
       "   'and',\n",
       "   'e m',\n",
       "   'ene',\n",
       "   'o t',\n",
       "   'o c',\n",
       "   'ste',\n",
       "   'al ',\n",
       "   'e h',\n",
       "   'nto',\n",
       "   'o l',\n",
       "   ' le',\n",
       "   ' ti',\n",
       "   're ',\n",
       "   'a t',\n",
       "   'tar',\n",
       "   'ría',\n",
       "   'ás ',\n",
       "   'nta',\n",
       "   's l',\n",
       "   's s',\n",
       "   's m',\n",
       "   'ada',\n",
       "   ' to',\n",
       "   ' pu',\n",
       "   'pre',\n",
       "   'ión',\n",
       "   'end',\n",
       "   ' vi',\n",
       "   'El ',\n",
       "   'o m',\n",
       "   'sto',\n",
       "   'des',\n",
       "   'ten',\n",
       "   'tie',\n",
       "   'ida',\n",
       "   'ued',\n",
       "   'r e',\n",
       "   ' in',\n",
       "   'le ',\n",
       "   ' tr',\n",
       "   'e q',\n",
       "   'ido',\n",
       "   'dos',\n",
       "   'a v',\n",
       "   ' cu',\n",
       "   'dad',\n",
       "   'ran',\n",
       "   'Est',\n",
       "   'ón ',\n",
       "   'uer',\n",
       "   'aci',\n",
       "   'más',\n",
       "   'qui',\n",
       "   'a n',\n",
       "   'mo ',\n",
       "   'pue',\n",
       "   'go ',\n",
       "   'r a',\n",
       "   'ció',\n",
       "   ' sa',\n",
       "   'tan',\n",
       "   'uie',\n",
       "   'e n',\n",
       "   'hab',\n",
       "   'La ',\n",
       "   'tá ',\n",
       "   'ver',\n",
       "   'ace',\n",
       "   'o h',\n",
       "   'n a',\n",
       "   'n p',\n",
       "   'ué ',\n",
       "   'rec',\n",
       "   'n c',\n",
       "   'su ',\n",
       "   'n d',\n",
       "   'cer',\n",
       "   ' ta',\n",
       "   's t',\n",
       "   'nos',\n",
       "   ' má',\n",
       "   'ist']},\n",
       " 'fra': {'50': [' de',\n",
       "   'es ',\n",
       "   'de ',\n",
       "   'le ',\n",
       "   ' pa',\n",
       "   'ent',\n",
       "   ' qu',\n",
       "   'nt ',\n",
       "   'ne ',\n",
       "   're ',\n",
       "   'us ',\n",
       "   ' le',\n",
       "   'que',\n",
       "   'est',\n",
       "   'e p',\n",
       "   'ous',\n",
       "   ' la',\n",
       "   'is ',\n",
       "   'e d',\n",
       "   'ue ',\n",
       "   'e s',\n",
       "   'st ',\n",
       "   'pas',\n",
       "   'lle',\n",
       "   'as ',\n",
       "   'ais',\n",
       "   'e l',\n",
       "   'it ',\n",
       "   'la ',\n",
       "   ' un',\n",
       "   's d',\n",
       "   'er ',\n",
       "   'Je ',\n",
       "   'e c',\n",
       "   'our',\n",
       "   'on ',\n",
       "   'ns ',\n",
       "   's p',\n",
       "   ' ne',\n",
       "   ' à ',\n",
       "   ' co',\n",
       "   ' es',\n",
       "   'te ',\n",
       "   'e n',\n",
       "   ' ma',\n",
       "   'les',\n",
       "   'ait',\n",
       "   ' vo',\n",
       "   ' so',\n",
       "   'e t'],\n",
       "  '100': [' de',\n",
       "   'es ',\n",
       "   'de ',\n",
       "   'le ',\n",
       "   ' pa',\n",
       "   'ent',\n",
       "   ' qu',\n",
       "   'nt ',\n",
       "   'ne ',\n",
       "   're ',\n",
       "   'us ',\n",
       "   ' le',\n",
       "   'que',\n",
       "   'est',\n",
       "   'e p',\n",
       "   'ous',\n",
       "   ' la',\n",
       "   'is ',\n",
       "   'e d',\n",
       "   'ue ',\n",
       "   'e s',\n",
       "   'st ',\n",
       "   'pas',\n",
       "   'lle',\n",
       "   'as ',\n",
       "   'ais',\n",
       "   'e l',\n",
       "   'it ',\n",
       "   'la ',\n",
       "   ' un',\n",
       "   's d',\n",
       "   'er ',\n",
       "   'Je ',\n",
       "   'e c',\n",
       "   'our',\n",
       "   'on ',\n",
       "   'ns ',\n",
       "   's p',\n",
       "   ' ne',\n",
       "   ' à ',\n",
       "   ' co',\n",
       "   ' es',\n",
       "   'te ',\n",
       "   'e n',\n",
       "   ' ma',\n",
       "   'les',\n",
       "   'ait',\n",
       "   ' vo',\n",
       "   ' so',\n",
       "   'e t',\n",
       "   ' en',\n",
       "   'ur ',\n",
       "   'e m',\n",
       "   ' po',\n",
       "   ' ce',\n",
       "   'en ',\n",
       "   'men',\n",
       "   ' se',\n",
       "   's l',\n",
       "   'e v',\n",
       "   'un ',\n",
       "   's a',\n",
       "   'vou',\n",
       "   'tre',\n",
       "   ' fa',\n",
       "   'ce ',\n",
       "   'e q',\n",
       "   ' pe',\n",
       "   'Il ',\n",
       "   'me ',\n",
       "   'son',\n",
       "   't d',\n",
       "   'ont',\n",
       "   ' pr',\n",
       "   'ien',\n",
       "   'ire',\n",
       "   ' a ',\n",
       "   'ant',\n",
       "   'se ',\n",
       "   's s',\n",
       "   't p',\n",
       "   'une',\n",
       "   ' to',\n",
       "   ' tr',\n",
       "   ' ch',\n",
       "   'rai',\n",
       "   ' mo',\n",
       "   'par',\n",
       "   ' re',\n",
       "   ' su',\n",
       "   'e e',\n",
       "   'ai ',\n",
       "   's c',\n",
       "   'eur',\n",
       "   'ons',\n",
       "   'ion',\n",
       "   'fai',\n",
       "   ' da',\n",
       "   't l',\n",
       "   'pou'],\n",
       "  '200': [' de',\n",
       "   'es ',\n",
       "   'de ',\n",
       "   'le ',\n",
       "   ' pa',\n",
       "   'ent',\n",
       "   ' qu',\n",
       "   'nt ',\n",
       "   'ne ',\n",
       "   're ',\n",
       "   'us ',\n",
       "   ' le',\n",
       "   'que',\n",
       "   'est',\n",
       "   'e p',\n",
       "   'ous',\n",
       "   ' la',\n",
       "   'is ',\n",
       "   'e d',\n",
       "   'ue ',\n",
       "   'e s',\n",
       "   'st ',\n",
       "   'pas',\n",
       "   'lle',\n",
       "   'as ',\n",
       "   'ais',\n",
       "   'e l',\n",
       "   'it ',\n",
       "   'la ',\n",
       "   ' un',\n",
       "   's d',\n",
       "   'er ',\n",
       "   'Je ',\n",
       "   'e c',\n",
       "   'our',\n",
       "   'on ',\n",
       "   'ns ',\n",
       "   's p',\n",
       "   ' ne',\n",
       "   ' à ',\n",
       "   ' co',\n",
       "   ' es',\n",
       "   'te ',\n",
       "   'e n',\n",
       "   ' ma',\n",
       "   'les',\n",
       "   'ait',\n",
       "   ' vo',\n",
       "   ' so',\n",
       "   'e t',\n",
       "   ' en',\n",
       "   'ur ',\n",
       "   'e m',\n",
       "   ' po',\n",
       "   ' ce',\n",
       "   'en ',\n",
       "   'men',\n",
       "   ' se',\n",
       "   's l',\n",
       "   'e v',\n",
       "   'un ',\n",
       "   's a',\n",
       "   'vou',\n",
       "   'tre',\n",
       "   ' fa',\n",
       "   'ce ',\n",
       "   'e q',\n",
       "   ' pe',\n",
       "   'Il ',\n",
       "   'me ',\n",
       "   'son',\n",
       "   't d',\n",
       "   'ont',\n",
       "   ' pr',\n",
       "   'ien',\n",
       "   'ire',\n",
       "   ' a ',\n",
       "   'ant',\n",
       "   'se ',\n",
       "   's s',\n",
       "   't p',\n",
       "   'une',\n",
       "   ' to',\n",
       "   ' tr',\n",
       "   ' ch',\n",
       "   'rai',\n",
       "   ' mo',\n",
       "   'par',\n",
       "   ' re',\n",
       "   ' su',\n",
       "   'e e',\n",
       "   'ai ',\n",
       "   's c',\n",
       "   'eur',\n",
       "   'ons',\n",
       "   'ion',\n",
       "   'fai',\n",
       "   ' da',\n",
       "   't l',\n",
       "   'pou',\n",
       "   'e a',\n",
       "   ' au',\n",
       "   'eux',\n",
       "   ' no',\n",
       "   ' sa',\n",
       "   'il ',\n",
       "   'e f',\n",
       "   ' av',\n",
       "   ' pl',\n",
       "   ' me',\n",
       "   'ans',\n",
       "   's e',\n",
       "   'ux ',\n",
       "   'mme',\n",
       "   ' di',\n",
       "   'mai',\n",
       "   'des',\n",
       "   ' te',\n",
       "   'res',\n",
       "   'Tom',\n",
       "   'tu ',\n",
       "   'tou',\n",
       "   'con',\n",
       "   'ell',\n",
       "   'ez ',\n",
       "   'et ',\n",
       "   'end',\n",
       "   'r l',\n",
       "   'qui',\n",
       "   'omm',\n",
       "   'a p',\n",
       "   'onn',\n",
       "   'che',\n",
       "   'n d',\n",
       "   'ut ',\n",
       "   'om ',\n",
       "   'tai',\n",
       "   'ill',\n",
       "   'nte',\n",
       "   'ui ',\n",
       "   'e r',\n",
       "   'tes',\n",
       "   'air',\n",
       "   ' je',\n",
       "   'ren',\n",
       "   'dan',\n",
       "   'out',\n",
       "   'sse',\n",
       "   'ain',\n",
       "   ' dé',\n",
       "   ' du',\n",
       "   ' ét',\n",
       "   'vai',\n",
       "   'uis',\n",
       "   'je ',\n",
       "   'ouv',\n",
       "   'ir ',\n",
       "   'eme',\n",
       "   'com',\n",
       "   ' vi',\n",
       "   'ens',\n",
       "   'ure',\n",
       "   's m',\n",
       "   's t',\n",
       "   'oir',\n",
       "   'ter',\n",
       "   'ère',\n",
       "   'tte',\n",
       "   ' et',\n",
       "   'Ell',\n",
       "   'ave',\n",
       "   'tio',\n",
       "   'e j',\n",
       "   'n p',\n",
       "   'ois',\n",
       "   'nou',\n",
       "   'rie',\n",
       "   'cha',\n",
       "   's q',\n",
       "   'té ',\n",
       "   'ort',\n",
       "   'tra',\n",
       "   'r d',\n",
       "   's v',\n",
       "   't a',\n",
       "   's n',\n",
       "   'ava',\n",
       "   'nne',\n",
       "   ' na',\n",
       "   'lai',\n",
       "   'uel',\n",
       "   'rs ',\n",
       "   'au ',\n",
       "   'du ',\n",
       "   'ie ',\n",
       "   'and',\n",
       "   'ses',\n",
       "   'cou',\n",
       "   ' ve',\n",
       "   ' do']},\n",
       " 'por': {'50': [' de',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'de ',\n",
       "   'ão ',\n",
       "   ' co',\n",
       "   'ue ',\n",
       "   ' es',\n",
       "   'os ',\n",
       "   'do ',\n",
       "   'om ',\n",
       "   'est',\n",
       "   ' se',\n",
       "   'as ',\n",
       "   'em ',\n",
       "   ' o ',\n",
       "   ' a ',\n",
       "   ' um',\n",
       "   'ent',\n",
       "   'nte',\n",
       "   'ar ',\n",
       "   'ra ',\n",
       "   'Tom',\n",
       "   ' pa',\n",
       "   'não',\n",
       "   ' nã',\n",
       "   'o d',\n",
       "   'com',\n",
       "   'ocê',\n",
       "   'er ',\n",
       "   'to ',\n",
       "   'e e',\n",
       "   'te ',\n",
       "   ' me',\n",
       "   ' te',\n",
       "   'ou ',\n",
       "   ' po',\n",
       "   ' é ',\n",
       "   'a d',\n",
       "   'eu ',\n",
       "   'sta',\n",
       "   'ia ',\n",
       "   'Eu ',\n",
       "   'par',\n",
       "   'da ',\n",
       "   ' ca',\n",
       "   'ara',\n",
       "   'cê ',\n",
       "   'se ',\n",
       "   'a c'],\n",
       "  '100': [' de',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'de ',\n",
       "   'ão ',\n",
       "   ' co',\n",
       "   'ue ',\n",
       "   ' es',\n",
       "   'os ',\n",
       "   'do ',\n",
       "   'om ',\n",
       "   'est',\n",
       "   ' se',\n",
       "   'as ',\n",
       "   'em ',\n",
       "   ' o ',\n",
       "   ' a ',\n",
       "   ' um',\n",
       "   'ent',\n",
       "   'nte',\n",
       "   'ar ',\n",
       "   'ra ',\n",
       "   'Tom',\n",
       "   ' pa',\n",
       "   'não',\n",
       "   ' nã',\n",
       "   'o d',\n",
       "   'com',\n",
       "   'ocê',\n",
       "   'er ',\n",
       "   'to ',\n",
       "   'e e',\n",
       "   'te ',\n",
       "   ' me',\n",
       "   ' te',\n",
       "   'ou ',\n",
       "   ' po',\n",
       "   ' é ',\n",
       "   'a d',\n",
       "   'eu ',\n",
       "   'sta',\n",
       "   'ia ',\n",
       "   'Eu ',\n",
       "   'par',\n",
       "   'da ',\n",
       "   ' ca',\n",
       "   'ara',\n",
       "   'cê ',\n",
       "   'se ',\n",
       "   'a c',\n",
       "   ' pr',\n",
       "   'o p',\n",
       "   'ndo',\n",
       "   'o e',\n",
       "   'e a',\n",
       "   'ma ',\n",
       "   ' fa',\n",
       "   'um ',\n",
       "   'con',\n",
       "   'o q',\n",
       "   ' ma',\n",
       "   'sso',\n",
       "   ' vo',\n",
       "   'or ',\n",
       "   'ado',\n",
       "   'a e',\n",
       "   ' pe',\n",
       "   ' di',\n",
       "   'o a',\n",
       "   's d',\n",
       "   'o c',\n",
       "   'es ',\n",
       "   'e d',\n",
       "   ' do',\n",
       "   ' no',\n",
       "   'uma',\n",
       "   'a m',\n",
       "   'a p',\n",
       "   'ria',\n",
       "   'men',\n",
       "   'o m',\n",
       "   'o s',\n",
       "   'a a',\n",
       "   'is ',\n",
       "   'pre',\n",
       "   ' em',\n",
       "   'and',\n",
       "   'le ',\n",
       "   'voc',\n",
       "   'ta ',\n",
       "   'e v',\n",
       "   'ro ',\n",
       "   'e c',\n",
       "   'ant',\n",
       "   'so ',\n",
       "   'ito',\n",
       "   'la ',\n",
       "   ' To',\n",
       "   's e',\n",
       "   ' mu'],\n",
       "  '200': [' de',\n",
       "   ' qu',\n",
       "   'que',\n",
       "   'de ',\n",
       "   'ão ',\n",
       "   ' co',\n",
       "   'ue ',\n",
       "   ' es',\n",
       "   'os ',\n",
       "   'do ',\n",
       "   'om ',\n",
       "   'est',\n",
       "   ' se',\n",
       "   'as ',\n",
       "   'em ',\n",
       "   ' o ',\n",
       "   ' a ',\n",
       "   ' um',\n",
       "   'ent',\n",
       "   'nte',\n",
       "   'ar ',\n",
       "   'ra ',\n",
       "   'Tom',\n",
       "   ' pa',\n",
       "   'não',\n",
       "   ' nã',\n",
       "   'o d',\n",
       "   'com',\n",
       "   'ocê',\n",
       "   'er ',\n",
       "   'to ',\n",
       "   'e e',\n",
       "   'te ',\n",
       "   ' me',\n",
       "   ' te',\n",
       "   'ou ',\n",
       "   ' po',\n",
       "   ' é ',\n",
       "   'a d',\n",
       "   'eu ',\n",
       "   'sta',\n",
       "   'ia ',\n",
       "   'Eu ',\n",
       "   'par',\n",
       "   'da ',\n",
       "   ' ca',\n",
       "   'ara',\n",
       "   'cê ',\n",
       "   'se ',\n",
       "   'a c',\n",
       "   ' pr',\n",
       "   'o p',\n",
       "   'ndo',\n",
       "   'o e',\n",
       "   'e a',\n",
       "   'ma ',\n",
       "   ' fa',\n",
       "   'um ',\n",
       "   'con',\n",
       "   'o q',\n",
       "   ' ma',\n",
       "   'sso',\n",
       "   ' vo',\n",
       "   'or ',\n",
       "   'ado',\n",
       "   'a e',\n",
       "   ' pe',\n",
       "   ' di',\n",
       "   'o a',\n",
       "   's d',\n",
       "   'o c',\n",
       "   'es ',\n",
       "   'e d',\n",
       "   ' do',\n",
       "   ' no',\n",
       "   'uma',\n",
       "   'a m',\n",
       "   'a p',\n",
       "   'ria',\n",
       "   'men',\n",
       "   'o m',\n",
       "   'o s',\n",
       "   'a a',\n",
       "   'is ',\n",
       "   'pre',\n",
       "   ' em',\n",
       "   'and',\n",
       "   'le ',\n",
       "   'voc',\n",
       "   'ta ',\n",
       "   'e v',\n",
       "   'ro ',\n",
       "   'e c',\n",
       "   'ant',\n",
       "   'so ',\n",
       "   'ito',\n",
       "   'la ',\n",
       "   ' To',\n",
       "   's e',\n",
       "   ' mu',\n",
       "   's p',\n",
       "   'e p',\n",
       "   ' e ',\n",
       "   'res',\n",
       "   'inh',\n",
       "   'iss',\n",
       "   'ais',\n",
       "   'nto',\n",
       "   'Voc',\n",
       "   'mos',\n",
       "   'o t',\n",
       "   'tem',\n",
       "   'stá',\n",
       "   'ele',\n",
       "   'ess',\n",
       "   'a s',\n",
       "   ' na',\n",
       "   'me ',\n",
       "   's a',\n",
       "   'e s',\n",
       "   'tra',\n",
       "   ' da',\n",
       "   'ver',\n",
       "   'nha',\n",
       "   'tá ',\n",
       "   'am ',\n",
       "   'por',\n",
       "   'ora',\n",
       "   'e o',\n",
       "   'ont',\n",
       "   'sto',\n",
       "   'per',\n",
       "   ' re',\n",
       "   ' ve',\n",
       "   'tar',\n",
       "   'm e',\n",
       "   'no ',\n",
       "   ' el',\n",
       "   'e n',\n",
       "   'mai',\n",
       "   'a n',\n",
       "   'e t',\n",
       "   'sa ',\n",
       "   'sse',\n",
       "   'ida',\n",
       "   's c',\n",
       "   'e m',\n",
       "   'm a',\n",
       "   'na ',\n",
       "   'ava',\n",
       "   'r a',\n",
       "   ' fo',\n",
       "   'nho',\n",
       "   'ada',\n",
       "   'ho ',\n",
       "   's s',\n",
       "   'o n',\n",
       "   'e f',\n",
       "   'ha ',\n",
       "   'nta',\n",
       "   'Ele',\n",
       "   'uit',\n",
       "   'ei ',\n",
       "   'a f',\n",
       "   'ost',\n",
       "   'uer',\n",
       "   'rec',\n",
       "   'dos',\n",
       "   'end',\n",
       "   'nde',\n",
       "   'ica',\n",
       "   ' en',\n",
       "   ' sa',\n",
       "   'o f',\n",
       "   ' vi',\n",
       "   ' so',\n",
       "   'mui',\n",
       "   'Não',\n",
       "   ' ac',\n",
       "   ' in',\n",
       "   'ran',\n",
       "   'm p',\n",
       "   'ela',\n",
       "   'va ',\n",
       "   'ter',\n",
       "   'ade',\n",
       "   ' tr',\n",
       "   ' fi',\n",
       "   's n',\n",
       "   'm c',\n",
       "   'a t',\n",
       "   'car',\n",
       "   'era',\n",
       "   'u a',\n",
       "   'u n',\n",
       "   'des',\n",
       "   'faz',\n",
       "   ' is',\n",
       "   'm d',\n",
       "   'a v']},\n",
       " 'ita': {'50': ['re ',\n",
       "   'to ',\n",
       "   'on ',\n",
       "   ' co',\n",
       "   'no ',\n",
       "   ' di',\n",
       "   ' in',\n",
       "   ' a ',\n",
       "   'che',\n",
       "   'he ',\n",
       "   'are',\n",
       "   ' ch',\n",
       "   'di ',\n",
       "   'te ',\n",
       "   'o a',\n",
       "   'a c',\n",
       "   'la ',\n",
       "   'in ',\n",
       "   ' un',\n",
       "   ' no',\n",
       "   'o c',\n",
       "   'Tom',\n",
       "   ' la',\n",
       "   'o d',\n",
       "   'ent',\n",
       "   'and',\n",
       "   'Non',\n",
       "   ' pe',\n",
       "   'ato',\n",
       "   'e a',\n",
       "   'per',\n",
       "   'str',\n",
       "   ' è ',\n",
       "   ' qu',\n",
       "   'om ',\n",
       "   ' de',\n",
       "   'ono',\n",
       "   'non',\n",
       "   'cos',\n",
       "   'ost',\n",
       "   ' so',\n",
       "   'a s',\n",
       "   'sta',\n",
       "   'ta ',\n",
       "   'i s',\n",
       "   'e i',\n",
       "   'est',\n",
       "   'o s',\n",
       "   ' st',\n",
       "   'a d'],\n",
       "  '100': ['re ',\n",
       "   'to ',\n",
       "   'on ',\n",
       "   ' co',\n",
       "   'no ',\n",
       "   ' di',\n",
       "   ' in',\n",
       "   ' a ',\n",
       "   'che',\n",
       "   'he ',\n",
       "   'are',\n",
       "   ' ch',\n",
       "   'di ',\n",
       "   'te ',\n",
       "   'o a',\n",
       "   'a c',\n",
       "   'la ',\n",
       "   'in ',\n",
       "   ' un',\n",
       "   ' no',\n",
       "   'o c',\n",
       "   'Tom',\n",
       "   ' la',\n",
       "   'o d',\n",
       "   'ent',\n",
       "   'and',\n",
       "   'Non',\n",
       "   ' pe',\n",
       "   'ato',\n",
       "   'e a',\n",
       "   'per',\n",
       "   'str',\n",
       "   ' è ',\n",
       "   ' qu',\n",
       "   'om ',\n",
       "   ' de',\n",
       "   'ono',\n",
       "   'non',\n",
       "   'cos',\n",
       "   'ost',\n",
       "   ' so',\n",
       "   'a s',\n",
       "   'sta',\n",
       "   'ta ',\n",
       "   'i s',\n",
       "   'e i',\n",
       "   'est',\n",
       "   'o s',\n",
       "   ' st',\n",
       "   'a d',\n",
       "   'o i',\n",
       "   'le ',\n",
       "   'o p',\n",
       "   'e s',\n",
       "   ' fa',\n",
       "   'ere',\n",
       "   'e d',\n",
       "   'i a',\n",
       "   'nte',\n",
       "   'mo ',\n",
       "   ' an',\n",
       "   'il ',\n",
       "   'con',\n",
       "   ' il',\n",
       "   'e c',\n",
       "   'ti ',\n",
       "   ' pr',\n",
       "   'ell',\n",
       "   'ro ',\n",
       "   'ra ',\n",
       "   'ire',\n",
       "   'er ',\n",
       "   'a p',\n",
       "   'tto',\n",
       "   'na ',\n",
       "   ' ha',\n",
       "   'i c',\n",
       "   'un ',\n",
       "   'sto',\n",
       "   ' mi',\n",
       "   'ett',\n",
       "   ' se',\n",
       "   ' le',\n",
       "   'do ',\n",
       "   'gli',\n",
       "   'e l',\n",
       "   ' ma',\n",
       "   'ion',\n",
       "   ' pa',\n",
       "   'iam',\n",
       "   'amo',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'ess',\n",
       "   'era',\n",
       "   'lla',\n",
       "   'i p',\n",
       "   'io ',\n",
       "   'o l',\n",
       "   'e n'],\n",
       "  '200': ['re ',\n",
       "   'to ',\n",
       "   'on ',\n",
       "   ' co',\n",
       "   'no ',\n",
       "   ' di',\n",
       "   ' in',\n",
       "   ' a ',\n",
       "   'che',\n",
       "   'he ',\n",
       "   'are',\n",
       "   ' ch',\n",
       "   'di ',\n",
       "   'te ',\n",
       "   'o a',\n",
       "   'a c',\n",
       "   'la ',\n",
       "   'in ',\n",
       "   ' un',\n",
       "   ' no',\n",
       "   'o c',\n",
       "   'Tom',\n",
       "   ' la',\n",
       "   'o d',\n",
       "   'ent',\n",
       "   'and',\n",
       "   'Non',\n",
       "   ' pe',\n",
       "   'ato',\n",
       "   'e a',\n",
       "   'per',\n",
       "   'str',\n",
       "   ' è ',\n",
       "   ' qu',\n",
       "   'om ',\n",
       "   ' de',\n",
       "   'ono',\n",
       "   'non',\n",
       "   'cos',\n",
       "   'ost',\n",
       "   ' so',\n",
       "   'a s',\n",
       "   'sta',\n",
       "   'ta ',\n",
       "   'i s',\n",
       "   'e i',\n",
       "   'est',\n",
       "   'o s',\n",
       "   ' st',\n",
       "   'a d',\n",
       "   'o i',\n",
       "   'le ',\n",
       "   'o p',\n",
       "   'e s',\n",
       "   ' fa',\n",
       "   'ere',\n",
       "   'e d',\n",
       "   'i a',\n",
       "   'nte',\n",
       "   'mo ',\n",
       "   ' an',\n",
       "   'il ',\n",
       "   'con',\n",
       "   ' il',\n",
       "   'e c',\n",
       "   'ti ',\n",
       "   ' pr',\n",
       "   'ell',\n",
       "   'ro ',\n",
       "   'ra ',\n",
       "   'ire',\n",
       "   'er ',\n",
       "   'a p',\n",
       "   'tto',\n",
       "   'na ',\n",
       "   ' ha',\n",
       "   'i c',\n",
       "   'un ',\n",
       "   'sto',\n",
       "   ' mi',\n",
       "   'ett',\n",
       "   ' se',\n",
       "   ' le',\n",
       "   'do ',\n",
       "   'gli',\n",
       "   'e l',\n",
       "   ' ma',\n",
       "   'ion',\n",
       "   ' pa',\n",
       "   'iam',\n",
       "   'amo',\n",
       "   'e p',\n",
       "   ' su',\n",
       "   'ess',\n",
       "   'era',\n",
       "   'lla',\n",
       "   'i p',\n",
       "   'io ',\n",
       "   'o l',\n",
       "   'e n',\n",
       "   ' ca',\n",
       "   'son',\n",
       "   ' si',\n",
       "   'a a',\n",
       "   'ei ',\n",
       "   ' pi',\n",
       "   'a m',\n",
       "   'ai ',\n",
       "   'ver',\n",
       "   'men',\n",
       "   'ha ',\n",
       "   'ia ',\n",
       "   'nda',\n",
       "   'i d',\n",
       "   'so ',\n",
       "   'i i',\n",
       "   'ne ',\n",
       "   ' ri',\n",
       "   ' da',\n",
       "   'tru',\n",
       "   'ndo',\n",
       "   ' al',\n",
       "   'que',\n",
       "   ' ve',\n",
       "   'uir',\n",
       "   'ann',\n",
       "   'rui',\n",
       "   'ate',\n",
       "   ' po',\n",
       "   'una',\n",
       "   'chi',\n",
       "   'zio',\n",
       "   ' mo',\n",
       "   'lo ',\n",
       "   'sa ',\n",
       "   'ran',\n",
       "   'ues',\n",
       "   ' vo',\n",
       "   'pre',\n",
       "   'a f',\n",
       "   'a i',\n",
       "   ' vi',\n",
       "   'tat',\n",
       "   'n v',\n",
       "   'Io ',\n",
       "   ' To',\n",
       "   'li ',\n",
       "   'del',\n",
       "   'par',\n",
       "   'nno',\n",
       "   'one',\n",
       "   'si ',\n",
       "   'nti',\n",
       "   'att',\n",
       "   'qua',\n",
       "   'ano',\n",
       "   'a l',\n",
       "   'tra',\n",
       "   'se ',\n",
       "   'all',\n",
       "   ' do',\n",
       "   'ali',\n",
       "   ' ne',\n",
       "   'oi ',\n",
       "   'ant',\n",
       "   'o n',\n",
       "   ' va',\n",
       "   'mi ',\n",
       "   ' me',\n",
       "   'olt',\n",
       "   ' tu',\n",
       "   'va ',\n",
       "   'man',\n",
       "   'ero',\n",
       "   'ete',\n",
       "   ' lo',\n",
       "   'tro',\n",
       "   'sse',\n",
       "   'gio',\n",
       "   'com',\n",
       "   'osa',\n",
       "   'ora',\n",
       "   'n a',\n",
       "   'ata',\n",
       "   'azi',\n",
       "   'far',\n",
       "   'eri',\n",
       "   'n s',\n",
       "   'o m',\n",
       "   'da ',\n",
       "   'oro',\n",
       "   'o u',\n",
       "   'n p',\n",
       "   'res',\n",
       "   'sso',\n",
       "   'pia',\n",
       "   'anc',\n",
       "   'pro',\n",
       "   'erc',\n",
       "   ' er']}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dictionary of the features for all the languages \n",
    "lang = ['eng','deu','spa','fra','por','ita']\n",
    "n_list = ['50','100','200']\n",
    "lang_trigrams = {}\n",
    "for l in lang:\n",
    "    lang_trigrams[l] = lang_features(n_list,l)\n",
    "    print(l)\n",
    "lang_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1345cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the lang_trigrams select list of unique trigrams i.e. final feature list\n",
    "features = {} #final feature list\n",
    "for n in n_list:\n",
    "    n_trigrams = []\n",
    "    for l in lang:\n",
    "            n_trigrams = n_trigrams + lang_trigrams[l][n]\n",
    "    features[n] = sorted(list(set(n_trigrams)))\n",
    "\n",
    "with open('data/features.json', 'w') as outfile:\n",
    "    json.dump(features, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "13fec0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' To', ' a ', ' an', ' au', ' be', ' ca', ' ch', ' co', ' da', ' de', ' di', ' do', ' ei', ' el', ' en', ' es', ' ge', ' ha', ' he', ' in', ' is', ' la', ' le', ' lo', ' ma', ' me', ' mi', ' ne', ' ni', ' no', ' nã', ' o ', ' of', ' pa', ' pe', ' po', ' qu', ' se', ' si', ' so', ' st', ' te', ' th', ' to', ' um', ' un', ' vo', ' wa', ' wi', ' yo', ' zu', ' à ', ' è ', ' é ', 'Eu ', 'Ich', 'Je ', 'Mar', 'Non', 'Tom', 'a c', 'a d', 'a e', 'a s', 'ach', 'ado', 'ais', 'ait', 'and', 'ar ', 'ara', 'are', 'ary', 'as ', 'at ', 'ato', 'ch ', 'che', 'cht', 'com', 'con', 'cos', 'cê ', 'd t', 'da ', 'das', 'de ', 'den', 'der', 'di ', 'die', 'do ', 'e a', 'e c', 'e d', 'e e', 'e i', 'e l', 'e n', 'e p', 'e s', 'e t', 'e w', 'ed ', 'ein', 'el ', 'em ', 'en ', 'ent', 'er ', 'ere', 'es ', 'est', 'eu ', 'gen', 'hat', 'he ', 'hen', 'her', 'ht ', 'i s', 'ia ', 'ich', 'ie ', 'ien', 'in ', 'ine', 'ing', 'is ', 'ist', 'it ', 'la ', 'le ', 'les', 'll ', 'lle', 'n d', 'na ', 'nd ', 'ne ', 'nen', 'ng ', 'nic', 'no ', 'non', 'ns ', 'nt ', 'nte', 'não', 'o a', 'o c', 'o d', 'o e', 'o s', 'ocê', 'om ', 'on ', 'ono', 'or ', 'os ', 'ost', 'ou ', 'our', 'ous', 'par', 'pas', 'per', 'que', 'ra ', 're ', 'ry ', 's a', 's d', 's p', 'sch', 'se ', 'st ', 'sta', 'str', 't d', 't t', 'ta ', 'te ', 'ten', 'ter', 'tha', 'the', 'thi', 'to ', 'ue ', 'un ', 'und', 'us ', 've ', 'was', 'you', 'ão ']\n"
     ]
    }
   ],
   "source": [
    "with open('data/features.json', encoding='utf-8') as data_file:\n",
    "    features = json.loads(data_file.read())\n",
    "print(features['50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "94edbdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(sent,feature_list):\n",
    "    \"Takes in a list trigram (sentence in trigram form) and returns a feature in vector form, given a list of features\"\n",
    "    vector = [sent.count(f) for f in feature_list]\n",
    "    return(vector)\n",
    "print(vectorize([' Ma', ' To', ' a ', ' an', ' au', \" Ma\", \"e t\",'ccc','ach','st ','ach'],features['50']))\n",
    "print(vectorize([' Ma', ' To', ' a ', ' an', ' au', \" Ma\", \"e t\",'ccc','ach','st ','ach'],features['100']))\n",
    "print(vectorize([\"aaa\", \"bbb\",'eee','aaa','ccc','aaa'],features['50']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0feddb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(name,number):\n",
    "    \"Creates the apporiate feature matrix\"\n",
    "    mat = pd.read_csv(\"data/{}.csv\".format(name))\n",
    "    mat.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "    \n",
    "    sent_list = mat['sent']\n",
    "    sent_clean = list(map(clean_text, sent_list))\n",
    "    sent_trigram = list(map(char_trigram,sent_clean))\n",
    "    lang = mat['lang']\n",
    "    \n",
    "    vectors = [vectorize(s,features[number]) for s in sent_trigram]\n",
    "    df = pd.DataFrame(vectors, columns=features[number])\n",
    "    \n",
    "    df['lang'] = lang\n",
    "    df.to_csv('data/features/{}_{}.csv'.format(name,number))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "641d033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>To</th>\n",
       "      <th>a</th>\n",
       "      <th>an</th>\n",
       "      <th>au</th>\n",
       "      <th>be</th>\n",
       "      <th>ca</th>\n",
       "      <th>ch</th>\n",
       "      <th>co</th>\n",
       "      <th>da</th>\n",
       "      <th>de</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>ue</th>\n",
       "      <th>un</th>\n",
       "      <th>und</th>\n",
       "      <th>us</th>\n",
       "      <th>ve</th>\n",
       "      <th>was</th>\n",
       "      <th>you</th>\n",
       "      <th>ão</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        To   a    an   au   be   ca   ch   co   da   de  ...  to   ue   un   \\\n",
       "0        0    0    0    0    0    1    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    1  ...    1    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    1  ...    0    0    0   \n",
       "3        0    0    0    0    0    1    0    0    0    0  ...    0    0    0   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "29995    0    0    0    0    0    0    0    0    0    2  ...    0    0    0   \n",
       "29996    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "29997    0    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
       "29998    0    1    0    0    1    0    0    0    0    0  ...    0    0    0   \n",
       "29999    0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "\n",
       "       und  us   ve   was  you  ão   lang  \n",
       "0        0    0    1    0    1    0   eng  \n",
       "1        0    0    0    0    0    0   spa  \n",
       "2        0    0    0    0    0    0   eng  \n",
       "3        0    0    0    0    1    0   eng  \n",
       "4        0    0    1    0    0    0   ita  \n",
       "...    ...  ...  ...  ...  ...  ...   ...  \n",
       "29995    0    0    0    0    0    0   spa  \n",
       "29996    0    0    0    0    1    0   eng  \n",
       "29997    0    0    0    0    0    0   eng  \n",
       "29998    0    0    0    0    0    0   fra  \n",
       "29999    0    0    0    0    0    0   spa  \n",
       "\n",
       "[30000 rows x 198 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create feature vector matrix size 50\n",
    "create_features('train','50')\n",
    "create_features('valid','50')\n",
    "create_features('test','50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1197e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ge</th>\n",
       "      <th>I</th>\n",
       "      <th>Ma</th>\n",
       "      <th>Sc</th>\n",
       "      <th>Si</th>\n",
       "      <th>To</th>\n",
       "      <th>a</th>\n",
       "      <th>ac</th>\n",
       "      <th>al</th>\n",
       "      <th>an</th>\n",
       "      <th>...</th>\n",
       "      <th>y w</th>\n",
       "      <th>you</th>\n",
       "      <th>zio</th>\n",
       "      <th>zu</th>\n",
       "      <th>ás</th>\n",
       "      <th>ão</th>\n",
       "      <th>ère</th>\n",
       "      <th>ía</th>\n",
       "      <th>ón</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 678 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ge   I    Ma   Sc   Si   To   a    ac   al   an  ...  y w  you  zio  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "29995    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "29996    0    0    0    0    0    0    0    0    0    0  ...    0    1    0   \n",
       "29997    0    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
       "29998    0    0    0    0    0    0    1    0    0    0  ...    0    0    0   \n",
       "29999    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       zu   ás   ão   ère  ía   ón   lang  \n",
       "0        0    0    0    0    0    0   eng  \n",
       "1        0    0    0    0    0    0   spa  \n",
       "2        0    0    0    0    0    0   eng  \n",
       "3        0    0    0    0    0    0   eng  \n",
       "4        0    0    0    0    0    0   ita  \n",
       "...    ...  ...  ...  ...  ...  ...   ...  \n",
       "29995    0    1    0    0    0    0   spa  \n",
       "29996    0    0    0    0    0    0   eng  \n",
       "29997    0    0    0    0    0    0   eng  \n",
       "29998    0    0    0    0    0    0   fra  \n",
       "29999    0    0    0    0    0    0   spa  \n",
       "\n",
       "[30000 rows x 678 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size 100\n",
    "create_features('train','100')\n",
    "create_features('valid','100')\n",
    "create_features('test','100')\n",
    "\n",
    "#size 200\n",
    "create_features('train','200')\n",
    "create_features('valid','200')\n",
    "create_features('test','200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc73f2",
   "metadata": {},
   "source": [
    "## ANN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "27be5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"Rerformates data so it is appropriate for Tensorflow DNNC\"\n",
    "    x = df.drop(['lang'], axis=1)\n",
    "    x.columns = ['trigram_'+str(col) for col in list(range(len(x.columns)))]\n",
    "    y = df['lang']\n",
    "    y = y.map({\"eng\": 0, \"deu\": 1, \"spa\": 2, \"fra\": 3, \"por\": 4, \"ita\": 5})\n",
    "    return (x,y)\n",
    "\n",
    "def get_data(feat_type):\n",
    "    \"Gets the training, valid and test data bases for a specific feature type\"\n",
    "    train = pd.read_csv(\"ANN_features/train_{}.csv\".format(feat_type),index_col=0)\n",
    "    valid = pd.read_csv(\"ANN_features/valid_{}.csv\".format(feat_type),index_col=0)\n",
    "    \n",
    "    train_red = train[0:50000] #Reduce number of records for testing purposes \n",
    "    valid_red = valid[0:5000]\n",
    "    (train_x,train_y) = prepare_data(train_red)\n",
    "    (valid_x,valid_y) = prepare_data(valid_red)\n",
    "    return (train_x,train_y), (valid_x,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "18b35d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input functions \n",
    "def train_input_fn(features, labels, batch_size =100):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size=100):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "\n",
    " #TensorFlow (2016) An Example of a DNNClassifier for the Iris dataset. [Source code]. WWW.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b8cd638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(hidden):\n",
    "    \n",
    "    # Feature columns describe how to use the input.\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    \"Fits a DNNC with the desired features and stores validation results \"\n",
    "    # Build a DNN.\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 196 nodes each.\n",
    "    hidden_units=hidden,\n",
    "    # 6 languages.\n",
    "    n_classes=6)\n",
    "\n",
    "    # Train the Model.\n",
    "    classifier.train(\n",
    "    input_fn=lambda:train_input_fn(train_x, train_y),\n",
    "    steps=1000)\n",
    "\n",
    "    predictions = list(classifier.predict(input_fn=lambda:eval_input_fn(valid_x,labels=None)))\n",
    "\n",
    "    pred_y = []\n",
    "    for p in predictions:\n",
    "        pred_y.append(p['class_ids'][0])\n",
    "        \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "39a35cea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ANN_features/train_50.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (train_x,train_y), (valid_x,valid_y) \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_x),\u001b[38;5;28mlen\u001b[39m(valid_x))\n\u001b[0;32m      3\u001b[0m train_x\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[124], line 11\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(feat_type)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(feat_type):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGets the training, valid and test data bases for a specific feature type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANN_features/train_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN_features/valid_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(feat_type),index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m     train_red \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m50000\u001b[39m] \u001b[38;5;66;03m#Reduce number of records for testing purposes \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ANN_features/train_50.csv'"
     ]
    }
   ],
   "source": [
    "(train_x,train_y), (valid_x,valid_y) = get_data('50')\n",
    "print(len(train_x),len(valid_x))\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: 50 hidden layer: [152]\n",
    "pred_y_50_1 = fit_model([152])\n",
    "print(classification_report(valid_y,pred_y_50_1,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_50_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: 50 hidden layer: [101]\n",
    "pred_y_50_2 = fit_model([101])\n",
    "print(classification_report(valid_y,pred_y_50_2,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_50_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: 50 hidden layer: [51]\n",
    "pred_y_50_3 = fit_model([51])\n",
    "print(classification_report(valid_y,pred_y_50_3,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_50_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ad13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,train_y), (valid_x,valid_y) = get_data('100')\n",
    "print(len(train_x),len(valid_x))\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: 100 hidden layer: [266]\n",
    "pred_y_100_1 = fit_model([266])\n",
    "print(classification_report(valid_y,pred_y_100_1,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_100_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: 100 hidden layer: [178]\n",
    "pred_y_100_2 = fit_model([178])\n",
    "print(classification_report(valid_y,pred_y_100_2,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_100_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dfaafb40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Feature: 100 hidden layer: [88]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pred_y_100_3 \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m88\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(valid_y,pred_y_100_3,digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(valid_y,pred_y_100_3))\n",
      "Cell \u001b[1;32mIn[117], line 5\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(hidden)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_model\u001b[39m(hidden):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Feature columns describe how to use the input.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     my_feature_columns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      6\u001b[0m         my_feature_columns\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mfeature_column\u001b[38;5;241m.\u001b[39mnumeric_column(key\u001b[38;5;241m=\u001b[39mkey))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFits a DNNC with the desired features and stores validation results \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "#Feature: 100 hidden layer: [88]\n",
    "pred_y_100_3 = fit_model([88])\n",
    "print(classification_report(valid_y,pred_y_100_3,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_100_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "503fec41",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ANN_features/train_200.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (train_x,train_y), (valid_x,valid_y) \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m200\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_x),\u001b[38;5;28mlen\u001b[39m(valid_x))\n\u001b[0;32m      3\u001b[0m train_x\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[115], line 11\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(feat_type)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(feat_type):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGets the training, valid and test data bases for a specific feature type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANN_features/train_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANN_features/valid_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(feat_type),index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m     train_red \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m50000\u001b[39m] \u001b[38;5;66;03m#Reduce number of records for testing purposes \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ANN_features/train_200.csv'"
     ]
    }
   ],
   "source": [
    "(train_x,train_y), (valid_x,valid_y) = get_data('200')\n",
    "print(len(train_x),len(valid_x))\n",
    "train_x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dbf14d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Feature: 200 hidden layer: [339]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pred_y_200_2 \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m339\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(valid_y,pred_y_200_2,digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(valid_y,pred_y_200_2))\n",
      "Cell \u001b[1;32mIn[117], line 5\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(hidden)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_model\u001b[39m(hidden):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Feature columns describe how to use the input.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     my_feature_columns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_x\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      6\u001b[0m         my_feature_columns\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mfeature_column\u001b[38;5;241m.\u001b[39mnumeric_column(key\u001b[38;5;241m=\u001b[39mkey))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFits a DNNC with the desired features and stores validation results \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "#Feature: 200 hidden layer: [339]\n",
    "pred_y_200_2 = fit_model([339])\n",
    "print(classification_report(valid_y,pred_y_200_2,digits=4))\n",
    "print(confusion_matrix(valid_y,pred_y_200_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336d52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
